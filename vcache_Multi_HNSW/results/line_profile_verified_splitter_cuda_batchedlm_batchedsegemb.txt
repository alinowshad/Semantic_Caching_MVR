args={'dataset': 'vCache/SemBenchmarkClassification', 'llm_col': 'response_llama_3_8b', 'splitter_checkpoint': '/data2/ali/checkpoints_words', 'splitter_device': 'cuda', 'delta': 0.02, 'candidate_k': 1, 'max_samples': 30, 'warmup_samples': 3, 'profile_samples': 10, 'sleep': 0.0, 'hf_cache_base': '/data2/ali/hf', 'output': 'results/line_profile_verified_splitter_cuda_batchedlm_batchedsegemb.txt'}
python=3.11.14 (main, Oct 21 2025, 18:31:21) [GCC 11.2.0]
cwd=/home/ali/vcahce

Timer unit: 0.001 s

Total time: 0.621059 s
File: /home/ali/vcahce/benchmarks/line_profile_verified_splitter.py
Function: main.<locals>.<lambda> at line 237

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   237         2        621.1    310.5    100.0          lambda: _run_loop(
   238         1          0.0      0.0      0.0              vcache=vcache,
   239         1          0.0      0.0      0.0              rows=rows_list[int(args.warmup_samples) :],
   240         1          0.0      0.0      0.0              llm_col=args.llm_col,
   241         1          0.0      0.0      0.0              profile_samples=int(args.profile_samples),
   242         1          0.0      0.0      0.0              sleep_s=float(args.sleep),
   243                                                   )

Total time: 0.620831 s
File: /home/ali/vcahce/vcache/main.py
Function: VCache.infer_with_cache_info at line 53

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    53                                               def infer_with_cache_info(
    54                                                   self,
    55                                                   prompt: str,
    56                                                   system_prompt: Optional[str] = None,
    57                                                   id_set: int = -1,
    58                                               ) -> Tuple[bool, str, EmbeddingMetadataObj, EmbeddingMetadataObj]:
    59                                                   """Infers a response and returns the cache hit status and metadata.
    60                                           
    61                                                   Args:
    62                                                       prompt (str): The prompt to create a response for.
    63                                                       system_prompt (Optional[str]): The optional system prompt to use
    64                                                           for the response. Overrides the system prompt in the
    65                                                           VCacheConfig if provided.
    66                                                       id_set (int): The set identifier for the embedding. This is used in the
    67                                                           benchmark to identify if the nearest neighbor is from the same set
    68                                                           (if the cached response is correct or incorrect).
    69                                           
    70                                                   Returns:
    71                                                       Tuple[bool, str, EmbeddingMetadataObj, EmbeddingMetadataObj]: A tuple containing the cache
    72                                                           hit status, the response, the metadata of the response and nearest neighbor metadata.
    73                                                   """
    74        10          0.0      0.0      0.0          if system_prompt is None:
    75                                                       system_prompt = self.vcache_config.system_prompt
    76                                           
    77        10          0.0      0.0      0.0          if self.vcache_config.eviction_policy.is_evicting():
    78                                                       response = self.__generate_response(prompt, system_prompt)
    79                                                       return (
    80                                                           False,
    81                                                           response,
    82                                                           EmbeddingMetadataObj(embedding_id=-1, response=response, id_set=id_set),
    83                                                           EmbeddingMetadataObj(embedding_id=-1, response=response, id_set=id_set),
    84                                                       )
    85                                           
    86        20        618.1     30.9     99.6          is_cache_hit, response, nn_metadata = self.vcache_policy.process_request(
    87        10          0.0      0.0      0.0              prompt, system_prompt, id_set
    88                                                   )
    89                                           
    90        10          0.0      0.0      0.0          if nn_metadata is not None:
    91        10          0.0      0.0      0.0              self.vcache_config.eviction_policy.update_eviction_metadata(nn_metadata)
    92                                           
    93        10          0.0      0.0      0.0          nn_metadata_copy: Optional[EmbeddingMetadataObj] = (
    94        10          2.5      0.3      0.4              copy.deepcopy(nn_metadata) if nn_metadata is not None else None
    95                                                   )
    96                                           
    97        10          0.0      0.0      0.0          if self.vcache_config.eviction_policy.ready_to_evict(self.vcache_policy.cache):
    98                                                       self.vcache_config.eviction_policy.evict(self.vcache_policy.cache)
    99                                           
   100        10          0.0      0.0      0.0          if is_cache_hit:
   101                                                       return is_cache_hit, response, nn_metadata_copy, nn_metadata_copy
   102                                                   else:
   103        10          0.0      0.0      0.0              return (
   104        10          0.0      0.0      0.0                  is_cache_hit,
   105        10          0.0      0.0      0.0                  response,
   106        10          0.1      0.0      0.0                  EmbeddingMetadataObj(embedding_id=-1, response=response, id_set=id_set),
   107        10          0.0      0.0      0.0                  nn_metadata_copy,
   108                                                       )

Total time: 0.139513 s
File: /home/ali/vcahce/vcache/vcache_core/cache/cache.py
Function: Cache.get_knn at line 65

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    65                                               def get_knn(self, prompt: str, k: int) -> List[tuple[float, int]]:
    66                                                   """Gets k-nearest neighbors for a given prompt.
    67                                           
    68                                                   Args:
    69                                                       prompt (str): The prompt to get the k-nearest neighbors for.
    70                                                       k (int): The number of nearest neighbors to retrieve.
    71                                           
    72                                                   Returns:
    73                                                       List[tuple[float, int]]: A list of tuples, each containing a
    74                                                       similarity score and an embedding ID.
    75                                                   """
    76        10        138.1     13.8     99.0          embedding = self.embedding_engine.get_embedding(prompt)
    77        10          1.4      0.1      1.0          return self.embedding_store.get_knn(embedding, k)

Total time: 0.138026 s
File: /home/ali/vcahce/vcache/vcache_core/cache/embedding_engine/strategies/bge.py
Function: BGEEmbeddingEngine.get_embedding at line 16

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    16                                               @override
    17                                               def get_embedding(self, text: str) -> List[float]:
    18                                                   """
    19                                                   Get the embedding for the given text using the underlying BGE model.
    20                                           
    21                                                   Args:
    22                                                       text: The text to get the embedding for.
    23                                           
    24                                                   Returns:
    25                                                       The embedding of the text as a list of floats.
    26                                                   """
    27                                                   # EmbeddingModel.get_embedding returns a numpy array, we convert to list
    28        10        137.8     13.8     99.8          embedding = self.embedding_model.get_embedding(text)
    29        10          0.3      0.0      0.2          return embedding.tolist()

Total time: 0.001301 s
File: /home/ali/vcahce/vcache/vcache_core/cache/embedding_store/vector_db/strategies/hnsw_lib.py
Function: HNSWLibVectorDB.get_knn at line 80

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    80                                               def get_knn(self, embedding: List[float], k: int) -> List[tuple[float, int]]:
    81                                                   """Gets k-nearest neighbors for a given embedding.
    82                                           
    83                                                   Args:
    84                                                       embedding (List[float]): The query embedding vector.
    85                                                       k (int): The number of nearest neighbors to return.
    86                                           
    87                                                   Returns:
    88                                                       List[tuple[float, int]]: A list of tuples containing similarity
    89                                                       scores and embedding IDs.
    90                                                   """
    91        10          0.0      0.0      1.4          if self.index is None:
    92                                                       return []
    93        10          0.0      0.0      1.7          k_ = min(k, self.embedding_count)
    94        10          0.0      0.0      0.5          if k_ == 0:
    95                                                       return []
    96        10          0.7      0.1     52.3          ids, similarities = self.index.knn_query(embedding, k=k_)
    97        10          0.1      0.0      5.1          metric_type = self.similarity_metric_type.value
    98        20          0.4      0.0     31.0          similarity_scores = [
    99        10          0.0      0.0      2.5              self.transform_similarity_score(sim, metric_type) for sim in similarities[0]
   100                                                   ]
   101        10          0.0      0.0      3.5          id_list = [int(id) for id in ids[0]]
   102        10          0.0      0.0      1.9          return list(zip(similarity_scores, id_list))

Total time: 0.0326758 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/AdaptedPointerNetworkPolicy.py
Function: CrossAttentionBlock.forward at line 27

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    27                                               def forward(
    28                                                   self,
    29                                                   emb_a: torch.Tensor,
    30                                                   emb_b: torch.Tensor,
    31                                                   pad_mask_a: torch.Tensor | None = None,
    32                                                   pad_mask_b: torch.Tensor | None = None,
    33                                               ) -> tuple[torch.Tensor, torch.Tensor]:
    34                                                   # A attends to B
    35        20         14.0      0.7     42.9          attn_a, _ = self.mha_ab(query=emb_a, key=emb_b, value=emb_b, key_padding_mask=pad_mask_b)
    36        20          1.3      0.1      4.1          a_out = self.norm_ab(emb_a + attn_a)
    37        20          3.3      0.2     10.0          a_out = self.norm_ab(a_out + self.ff_ab(a_out))
    38                                           
    39                                                   # B attends to A
    40        20          9.8      0.5     30.0          attn_b, _ = self.mha_ba(query=emb_b, key=emb_a, value=emb_a, key_padding_mask=pad_mask_a)
    41        20          1.1      0.1      3.4          b_out = self.norm_ba(emb_b + attn_b)
    42        20          3.1      0.2      9.5          b_out = self.norm_ba(b_out + self.ff_ba(b_out))
    43                                           
    44        20          0.0      0.0      0.0          return a_out, b_out

Total time: 0.124239 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/AdaptedPointerNetworkPolicy.py
Function: AdaptedPointerNetworkPolicy.forward at line 179

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   179                                               def forward(self, td, env=None, phase: str = "train", select_best: bool = False, **kwargs):
   180                                                   """与 RL4CO 的 REINFORCE 调用约定兼容的入口。
   181                                                   根据阶段/标志选择采样或贪心解码，返回包含 reward 与 log_likelihood 的字典。
   182                                                   """
   183                                                  
   184        10          0.0      0.0      0.0          decode_override = kwargs.pop("decode_type", None)
   185        10          0.0      0.0      0.0          if decode_override is not None:
   186        10          0.0      0.0      0.0              decode_type = decode_override
   187                                                   else:
   188                                                       decode_type = "greedy" if (select_best or phase != "train") else "sampling"
   189                                           
   190        10        111.9     11.2     90.0          actions, log_likelihood, info = self._forward(td, decode_type=decode_type, **kwargs)
   191                                                   # 计算奖励
   192        10          0.1      0.0      0.0          used_env = env if env is not None else getattr(self, 'env', None)
   193        10          0.0      0.0      0.0          if used_env is None:
   194                                                       raise RuntimeError("Environment instance is required to compute reward.")
   195        10         12.3      1.2      9.9          reward = used_env.get_reward(td, actions)
   196                                           
   197        10          0.0      0.0      0.0          return {
   198        10          0.0      0.0      0.0              "actions": actions,
   199        10          0.0      0.0      0.0              "log_likelihood": log_likelihood,
   200        10          0.0      0.0      0.0              "reward": reward,
   201        10          0.0      0.0      0.0              "info": info,
   202                                                   }

Total time: 0.110322 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/AdaptedPointerNetworkPolicy.py
Function: AdaptedPointerNetworkPolicy._forward at line 204

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   204                                               def _forward(self, td, decode_type="sampling", **kwargs):
   205                                                   """
   206                                                   实现自回归解码的核心逻辑 。
   207                                                   """
   208                                                   # Ignore legacy debug flag if provided (no-op)
   209        10          0.0      0.0      0.0          kwargs.pop("debug", None)
   210                                                  
   211                                                
   212        10          0.4      0.0      0.4          target_device = next(self.lm.parameters()).device
   213        10          0.0      0.0      0.0          if getattr(self, "_current_module_device", None) != target_device:
   214                                                       self.to(target_device)
   215                                                       self._current_module_device = target_device
   216        10          0.2      0.0      0.2          self._device = target_device
   217                                                   
   218                                              
   219        10          0.0      0.0      0.0          try:
   220        10          2.6      0.3      2.4              td = td.to(target_device)
   221                                                   except Exception:
   222                                                       for k in ['token_embeddings_a','token_embeddings_b','attention_mask_a','attention_mask_b',
   223                                                                 'input_ids_a','input_ids_b','length_a','length_b']:
   224                                                           if k in td and isinstance(td[k], torch.Tensor):
   225                                                               td[k] = td[k].to(target_device)
   226        10          0.2      0.0      0.2          self._init_punctuation_ids(td['input_ids_a'].device)
   227                                                   
   228                                                
   229        10          0.1      0.0      0.1          input_dim = td['token_embeddings_a'].size(-1)
   230        10          0.1      0.0      0.0          if self.input_proj is None:
   231                                                       with torch.inference_mode(False):
   232                                                           if input_dim == self.hidden_dim:
   233                                                               self.input_proj = nn.Identity()
   234                                                           else:
   235                                                               self.input_proj = nn.Linear(input_dim, self.hidden_dim)
   236                                                       self.input_proj.to(self.device)
   237                                           
   238        10          0.2      0.0      0.2          embedded_a = self.input_proj(td['token_embeddings_a'])
   239        10          0.2      0.0      0.2          embedded_b = self.input_proj(td['token_embeddings_b'])
   240                                                   
   241        10          0.8      0.1      0.7          mask_a = ~td['attention_mask_a'].bool()
   242        10          0.5      0.0      0.4          mask_b = ~td['attention_mask_b'].bool()
   243                                                   
   244        10          0.0      0.0      0.0          batch_size = embedded_a.size(0)
   245        10          0.0      0.0      0.0          seq_len_a = embedded_a.size(1)
   246        10          0.0      0.0      0.0          seq_len_b = embedded_b.size(1)
   247                                                   
   248                                                   # --- 2. Encoder (Cross-Attention) ---
   249        30          0.1      0.0      0.1          for layer in self.encoder_layers:
   250        40         33.1      0.8     30.0              embedded_a, embedded_b = layer(
   251        20          0.0      0.0      0.0                  emb_a=embedded_a, 
   252        20          0.0      0.0      0.0                  emb_b=embedded_b, 
   253        20          0.0      0.0      0.0                  pad_mask_a=mask_a, 
   254        20          0.0      0.0      0.0                  pad_mask_b=mask_b
   255                                                       )
   256        10          0.0      0.0      0.0          encoder_outputs_a = embedded_a
   257        10          0.0      0.0      0.0          encoder_outputs_b = embedded_b
   258                                                   
   259                                                   # --- 3. Decoder 初始化 ---
   260        10          0.4      0.0      0.4          decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)
   261        20          0.3      0.0      0.3          h, c = (torch.zeros(batch_size, self.decoder_cell.hidden_size, device=self.device),
   262        10          0.1      0.0      0.1                  torch.zeros(batch_size, self.decoder_cell.hidden_size, device=self.device))
   263                                                   
   264        10          0.0      0.0      0.0          pointers = []
   265        10          0.0      0.0      0.0          log_probs = []
   266                                                   
   267                                                   # 初始边界设为 0 (对应 [CLS])
   268        10          0.1      0.0      0.1          current_bA = torch.zeros(batch_size, dtype=torch.long, device=self.device)
   269        10          0.1      0.0      0.1          current_bB = torch.zeros(batch_size, dtype=torch.long, device=self.device)
   270                                           
   271                                                   # Precompute base punctuation mask (special end tokens are handled per-step)
   272        10          0.8      0.1      0.7          is_punct_base_a = torch.isin(td["input_ids_a"], self._valid_split_ids)
   273        10          0.4      0.0      0.4          is_punct_base_b = torch.isin(td["input_ids_b"], self._valid_split_ids)
   274                                           
   275                                                   # --- 4. 解码循环 ---
   276        50          0.0      0.0      0.0          for step in range(self.max_segments):
   277                                                       # (a) LSTM Step
   278        40          4.3      0.1      3.9              h, c = self.decoder_cell(decoder_input, (h, c))
   279        40          1.9      0.0      1.7              query_vec = self.attention_linear_decoder(h)
   280                                           
   281                                                       # Allow [SEP]/[EOS] only on the final step to prevent early degeneration.
   282        40          0.0      0.0      0.0              if step == self.max_segments - 1 and getattr(self, "_end_split_ids", None) is not None:
   283        10          0.7      0.1      0.6                  is_end_a = torch.isin(td["input_ids_a"], self._end_split_ids)
   284        10          0.4      0.0      0.4                  is_end_b = torch.isin(td["input_ids_b"], self._end_split_ids)
   285        10          0.2      0.0      0.2                  is_punct_global_a = is_punct_base_a | is_end_a
   286        10          0.1      0.0      0.1                  is_punct_global_b = is_punct_base_b | is_end_b
   287                                                       else:
   288        30          0.0      0.0      0.0                  is_punct_global_a = is_punct_base_a
   289        30          0.0      0.0      0.0                  is_punct_global_b = is_punct_base_b
   290                                           
   291                                                       #  指针 A 选择
   292        40          1.2      0.0      1.0              range_a = torch.arange(seq_len_a, device=self.device).expand(batch_size, -1)
   293                                                       
   294                                                       # 1. 必须是标点符号 (预计算结果)
   295                                                       # 2. 必须严格在当前边界之后 (range_a > current_bA)
   296                                                       # 3. 必须在有效长度内 (range_a < length_a)
   297                                                  
   298                                                       
   299        40          0.8      0.0      0.7              is_future_a = range_a > current_bA.unsqueeze(1)
   300        40          1.2      0.0      1.1              is_in_length_a = range_a < td['length_a'].unsqueeze(1)
   301        40          0.8      0.0      0.8              valid_slots_a = is_punct_global_a & is_future_a & is_in_length_a
   302        40          0.4      0.0      0.4              mask_ptr_a = ~valid_slots_a
   303                                           
   304        40          0.1      0.0      0.1              inp_a = query_vec.unsqueeze(2)
   305        40          4.7      0.1      4.3              ctx_a = self.attention_linear_encoder(encoder_outputs_a.permute(0, 2, 1))
   306        40          0.6      0.0      0.6              V_exp = self.V.unsqueeze(0).expand(batch_size, -1).unsqueeze(1)
   307        40          2.2      0.1      2.0              attn_scores_a = torch.bmm(V_exp, torch.tanh(inp_a + ctx_a)).squeeze(1)
   308                                                       
   309                                                  
   310        40          0.1      0.0      0.1              mask_fill_val = float('-inf') 
   311        40          1.1      0.0      1.0              attn_scores_a = attn_scores_a.masked_fill(mask_ptr_a, mask_fill_val)
   312                                           
   313                                                       # === 指针 B 选择 ===
   314        40          0.9      0.0      0.8              range_b = torch.arange(seq_len_b, device=self.device).expand(batch_size, -1)
   315                                                       
   316        40          0.6      0.0      0.6              is_future_b = range_b > current_bB.unsqueeze(1)
   317        40          1.2      0.0      1.1              is_in_length_b = range_b < td['length_b'].unsqueeze(1)
   318                                                       
   319        40          0.8      0.0      0.7              valid_slots_b = is_punct_global_b & is_future_b & is_in_length_b
   320        40          0.4      0.0      0.4              mask_ptr_b = ~valid_slots_b
   321                                           
   322        40          0.2      0.0      0.1              inp_b = query_vec.unsqueeze(2)
   323        40          4.0      0.1      3.6              ctx_b = self.attention_linear_encoder(encoder_outputs_b.permute(0, 2, 1))
   324        40          2.0      0.1      1.8              attn_scores_b = torch.bmm(V_exp, torch.tanh(inp_b + ctx_b)).squeeze(1)
   325        40          1.1      0.0      1.0              attn_scores_b = attn_scores_b.masked_fill(mask_ptr_b, mask_fill_val)
   326                                           
   327                                                       # === 采样/Argmax ===
   328        40          0.7      0.0      0.6              pointer_a = torch.zeros(batch_size, dtype=torch.long, device=self.device)
   329        40          0.4      0.0      0.4              pointer_b = torch.zeros(batch_size, dtype=torch.long, device=self.device)
   330        40          0.5      0.0      0.4              logp_a = torch.zeros(batch_size, device=self.device)
   331        40          0.4      0.0      0.4              logp_b = torch.zeros(batch_size, device=self.device)
   332                                           
   333        40          0.6      0.0      0.6              has_valid_a = valid_slots_a.any(dim=1)
   334        40          0.4      0.0      0.4              has_valid_b = valid_slots_b.any(dim=1)
   335                                           
   336                                                       # 处理 A
   337        40         12.6      0.3     11.4              if has_valid_a.any():
   338        27          0.0      0.0      0.0                  rows = has_valid_a
   339        27          0.0      0.0      0.0                  if decode_type == "sampling":
   340                                                               dist = Categorical(logits=attn_scores_a[rows])
   341                                                               act = dist.sample()
   342                                                               pointer_a[rows] = act
   343                                                               logp_a[rows] = dist.log_prob(act)
   344                                                           else:
   345        27          3.7      0.1      3.3                      pointer_a[rows] = torch.argmax(attn_scores_a[rows], dim=1)
   346                                           
   347        40          1.4      0.0      1.3              if (~has_valid_a).any():
   348        13          0.1      0.0      0.1                  rows = ~has_valid_a
   349                                                         
   350        13          1.8      0.1      1.7                  pointer_a[rows] = td['length_a'][rows] - 1
   351                                                        
   352                                           
   353                                                       # 处理 B
   354        40          0.9      0.0      0.8              if has_valid_b.any():
   355        24          0.0      0.0      0.0                  rows = has_valid_b
   356        24          0.0      0.0      0.0                  if decode_type == "sampling":
   357                                                               dist = Categorical(logits=attn_scores_b[rows])
   358                                                               act = dist.sample()
   359                                                               pointer_b[rows] = act
   360                                                               logp_b[rows] = dist.log_prob(act)
   361                                                           else:
   362        24          2.4      0.1      2.2                      pointer_b[rows] = torch.argmax(attn_scores_b[rows], dim=1)
   363                                                       
   364        40          1.2      0.0      1.1              if (~has_valid_b).any():
   365        16          0.1      0.0      0.1                  rows = ~has_valid_b
   366        16          1.9      0.1      1.8                  pointer_b[rows] = td['length_b'][rows] - 1
   367                                           
   368        40          0.9      0.0      0.8              pointers.append(torch.stack([pointer_a, pointer_b], dim=1))
   369        40          0.4      0.0      0.4              log_probs.append(logp_a + logp_b)
   370                                           
   371                                                       # ---------------------------------------------------------------------
   372                                                       # Fast feedback embedding: reuse token-level embeddings already computed
   373                                                       # outside the RL loop (MaxSimSplitter.split_pair_return_segments does one
   374                                                       # LM forward per text). We slice the span corresponding to the chosen
   375                                                       # boundary and mean-pool it, matching MaxSimEnv's "FAST VERSION":
   376                                                       #   real_start = (prev + 1) if prev > 0 else 1   # skip [CLS] at index 0
   377                                                       #   end        = pointer + 1                    # inclusive boundary token
   378                                                       # ---------------------------------------------------------------------
   379        40          0.7      0.0      0.6              token_emb_a = td["token_embeddings_a"]  # [B, L, H] on self.device already
   380        40          0.4      0.0      0.4              length_a = td["length_a"].long()  # [B]
   381                                           
   382        40          0.0      0.0      0.0              feedback_emb_a_list = []
   383        80          0.1      0.0      0.1              for b in range(batch_size):
   384        40          0.8      0.0      0.7                  s_a = int(current_bA[b].item())
   385        40          0.5      0.0      0.5                  e_a = int(pointer_a[b].item())
   386        40          0.5      0.0      0.4                  la = int(length_a[b].item())
   387        40          0.0      0.0      0.0                  la = max(1, la)
   388                                           
   389                                                           # Policy semantics: skip [CLS] (idx 0), boundary token is inclusive.
   390        40          0.0      0.0      0.0                  real_start_a = (s_a + 1) if s_a > 0 else 1
   391        40          0.0      0.0      0.0                  real_end_a = e_a + 1
   392                                           
   393                                                           # Clip to effective length to avoid pooling padded tail.
   394        40          0.0      0.0      0.0                  real_start_a = min(max(0, real_start_a), la)
   395        40          0.0      0.0      0.0                  real_end_a = min(max(0, real_end_a), la)
   396                                           
   397        40          0.0      0.0      0.0                  if real_end_a <= real_start_a:
   398                                                               # Match previous behavior: empty segment -> zeros embedding.
   399        14          0.1      0.0      0.1                      emb_val = torch.zeros(
   400         7          0.0      0.0      0.0                          token_emb_a.size(-1), device=token_emb_a.device, dtype=token_emb_a.dtype
   401                                                               )
   402                                                           else:
   403        33          1.0      0.0      0.9                      emb_val = token_emb_a[b, real_start_a:real_end_a, :].mean(dim=0)
   404                                           
   405        40          0.0      0.0      0.0                  feedback_emb_a_list.append(emb_val)
   406                                           
   407        40          0.9      0.0      0.8              feedback_tensor_a = torch.stack(feedback_emb_a_list, dim=0)  # [B, H]
   408                                                       
   409                                                       # 投影作为 LSTM 下一步输入
   410        40          0.7      0.0      0.6              decoder_input = self.input_proj(feedback_tensor_a)
   411                                           
   412                                                       # 更新状态
   413        40          0.0      0.0      0.0              current_bA = pointer_a
   414        40          0.1      0.0      0.0              current_bB = pointer_b
   415                                           
   416                                                
   417        10          0.2      0.0      0.2          actions = torch.stack(pointers, 1).view(batch_size, -1)
   418        10          0.3      0.0      0.3          log_likelihood = torch.stack(log_probs, 1).sum(dim=1)
   419                                           
   420        10          0.0      0.0      0.0          info = {}
   421                                           
   422        10          0.0      0.0      0.0          return actions, log_likelihood, info

Total time: 0.00553713 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/MaxSimEnv.py
Function: MaxSimEnv.raw_score_text at line 258

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   258                                               def raw_score_text(self,
   259                                                                query_tensor: torch.Tensor,
   260                                                                sub_corpus_embeddings: torch.Tensor,
   261                                                                query_weights: torch.Tensor,
   262                                                                corpus_weights: torch.Tensor,
   263                                                                times: int = 0) -> torch.Tensor:
   264                                                   """
   265                                                   计算query和corpus之间基于粗粒度和细粒度嵌入的加权相似度分数。
   266                                                   """
   267                                                  
   268        10          0.2      0.0      4.0          weights = F.softmax(self.score_weights_raw, dim=0)
   269        10          0.1      0.0      1.0          w_coarse = weights[0]
   270        10          0.0      0.0      0.5          w_fine_row = weights[1]
   271        10          0.0      0.0      0.6          w_fine_col = weights[2]
   272                                           
   273        10          0.1      0.0      1.5          query_full_vec = query_tensor[-1:, :]
   274        10          0.1      0.0      1.0          corpus_full_vec = sub_corpus_embeddings[-1:, :]
   275        10          1.2      0.1     21.5          coarse_grained_score = F.cosine_similarity(query_full_vec, corpus_full_vec).squeeze()
   276                                           
   277        10          0.1      0.0      1.5          query_sentence_vecs = query_tensor[:-1, :]
   278        10          0.1      0.0      1.0          corpus_sentence_vecs = sub_corpus_embeddings[:-1, :]
   279                                           
   280        10          0.0      0.0      0.3          if query_sentence_vecs.shape[0] > 0 and corpus_sentence_vecs.shape[0] > 0:
   281        10          0.8      0.1     14.3              query_norm = F.normalize(query_sentence_vecs, p=2, dim=-1)
   282        10          0.4      0.0      7.9              corpus_norm = F.normalize(corpus_sentence_vecs, p=2, dim=-1)
   283        10          0.5      0.1      9.5              cos_sim_matrix = torch.mm(query_norm, corpus_norm.T)
   284                                           
   285        10          0.3      0.0      5.3              max_cos_sim_row = torch.max(cos_sim_matrix, dim=1).values
   286        10          0.6      0.1     11.4              fine_grained_row_score = torch.sum(max_cos_sim_row * query_weights) / (torch.sum(query_weights) + 1e-8)
   287                                           
   288        10          0.2      0.0      2.8              max_cos_sim_col = torch.max(cos_sim_matrix, dim=0).values
   289        10          0.4      0.0      8.1              fine_grained_col_score = torch.sum(max_cos_sim_col * corpus_weights) / (torch.sum(corpus_weights) + 1e-8)
   290                                                   else:
   291                                                       fine_grained_row_score = torch.tensor(0.0, device=self.device)
   292                                                       fine_grained_col_score = torch.tensor(0.0, device=self.device)
   293                                           
   294                                                 
   295        30          0.3      0.0      4.7          final_score = (w_coarse * coarse_grained_score +
   296        10          0.1      0.0      1.7                         w_fine_row * fine_grained_row_score +
   297        10          0.1      0.0      1.4                         w_fine_col * fine_grained_col_score)
   298                                           
   299        10          0.0      0.0      0.1          return final_score

Total time: 0.0117351 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/MaxSimEnv.py
Function: MaxSimEnv._get_reward at line 301

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   301                                               def _get_reward(self, td: TensorDict, actions) -> torch.Tensor:
   302        10          0.0      0.0      0.2          batch_size = actions.shape[0]
   303        10          0.0      0.0      0.1          lm = self.reward_lm.model
   304        10          0.0      0.0      0.1          tok = self.reward_lm.tokenizer
   305        10          0.4      0.0      3.2          lm_device = next(lm.parameters()).device
   306        10          0.2      0.0      1.4          rewards = torch.zeros(batch_size, device=lm_device)
   307                                           
   308        10          0.2      0.0      1.3          input_ids_a = td["input_ids_a"]
   309        10          0.1      0.0      0.8          input_ids_b = td["input_ids_b"]
   310        10          0.1      0.0      0.9          length_a = td["length_a"].long()
   311        10          0.1      0.0      0.9          length_b = td["length_b"].long()
   312                                           
   313                                                   # Normalize actions -> pa_tensor/pb_tensor of shape [batch, max_segments]
   314        10          0.0      0.0      0.2          if isinstance(actions, torch.Tensor) and actions.dim() == 3 and actions.size(-1) == 2:
   315                                                       # shape [batch, K, 2] where last dim is [A, B]
   316                                                       pa_tensor = actions[:, :, 0]
   317                                                       pb_tensor = actions[:, :, 1]
   318        10          0.0      0.0      0.1          elif isinstance(actions, torch.Tensor) and actions.dim() == 2:
   319                                                       # shape [batch, 2*K] expected; policy currently outputs interleaved [A0,B0,A1,B1,...]
   320        10          0.0      0.0      0.2              if actions.size(1) != 2 * self.max_segments:
   321                                                           raise ValueError(
   322                                                               f"Unsupported actions shape for reward: {actions.shape}. "
   323                                                               f"Expected second dim = {2*self.max_segments} (2*max_segments)."
   324                                                           )
   325        10          0.1      0.0      1.0              pa_tensor = actions[:, 0::2]  # [A0, A1, ...]
   326        10          0.1      0.0      0.5              pb_tensor = actions[:, 1::2]  # [B0, B1, ...]
   327                                                   else:
   328                                                       raise ValueError(
   329                                                           f"Unsupported actions shape for reward: {getattr(actions, 'shape', None)}. "
   330                                                           f"Expected [batch, {2*self.max_segments}] or [batch, {self.max_segments}, 2]."
   331                                                       )
   332                                           
   333        20          0.0      0.0      0.2          for i in range(batch_size):
   334        10          0.6      0.1      5.0              pa = pa_tensor[i].tolist()
   335        10          0.3      0.0      2.9              pb = pb_tensor[i].tolist()
   336                                           
   337        10          0.2      0.0      1.5              la = int(length_a[i].item())
   338        10          0.1      0.0      1.1              lb = int(length_b[i].item())
   339        10          0.0      0.0      0.1              la = max(1, la)
   340        10          0.0      0.0      0.0              lb = max(1, lb)
   341                                           
   342        10          0.1      0.0      0.5              pa = [min(max(0, p), la - 1) for p in pa]
   343        10          0.0      0.0      0.3              pb = [min(max(0, p), lb - 1) for p in pb]
   344                                           
   345                                                       # NOTE: Align segmentation semantics with the policy.
   346                                                       # Policy treats pointers as "end boundary tokens" and skips [CLS] (index 0) when building segments:
   347                                                       #   real_start = prev_boundary + 1 (but starts from 1 when prev_boundary==0)
   348                                                       #   real_end   = pointer + 1      (inclusive of the boundary token)
   349        10          0.0      0.0      0.3              bounds_a = sorted(set(pa))
   350        10          0.0      0.0      0.1              bounds_b = sorted(set(pb))
   351                                           
   352                                                       # ===== ORIGINAL HEAVY LM FORWARD (保留为注释，作为兜底参考) =====
   353                                                       # seg_ids_a = []
   354                                                       # start = 0
   355                                                       # for p in bounds_a:
   356                                                       #     end = p + 1
   357                                                       #     if end > start:
   358                                                       #         seg_ids_a.append(input_ids_a[i, start:end])
   359                                                       #     start = end
   360                                                       # if start < la:
   361                                                       #     seg_ids_a.append(input_ids_a[i, start:la])
   362                                                       #
   363                                                       # seg_ids_b = []
   364                                                       # start = 0
   365                                                       # for p in bounds_b:
   366                                                       #     end = p + 1
   367                                                       #     if end > start:
   368                                                       #         seg_ids_b.append(input_ids_b[i, start:end])
   369                                                       #     start = end
   370                                                       # if start < lb:
   371                                                       #     seg_ids_b.append(input_ids_b[i, start:lb])
   372                                                       #
   373                                                       # if len(seg_ids_a) == 0 or len(seg_ids_b) == 0:
   374                                                       #     rewards[i] = 0.0
   375                                                       #     continue
   376                                                       #
   377                                                       # with torch.no_grad():
   378                                                       #     seg_emb_a_list = []
   379                                                       #     for s in seg_ids_a:
   380                                                       #         ids = s.unsqueeze(0).to(lm_device)
   381                                                       #         attn = torch.ones_like(ids, device=lm_device)
   382                                                       #         out = lm(ids, attention_mask=attn).last_hidden_state.mean(dim=1)
   383                                                       #         seg_emb_a_list.append(out.squeeze(0))
   384                                                       #     sentence_embeds_a = torch.stack(seg_emb_a_list, dim=0)
   385                                                       #
   386                                                       #     seg_emb_b_list = []
   387                                                       #     for s in seg_ids_b:
   388                                                       #         ids = s.unsqueeze(0).to(lm_device)
   389                                                       #         attn = torch.ones_like(ids, device=lm_device)
   390                                                       #         out = lm(ids, attention_mask=attn).last_hidden_state.mean(dim=1)
   391                                                       #         seg_emb_b_list.append(out.squeeze(0))
   392                                                       #     sentence_embeds_b = torch.stack(seg_emb_b_list, dim=0)
   393                                                       #
   394                                                       #     full_ids_a = input_ids_a[i, :la].unsqueeze(0).to(lm_device)
   395                                                       #     full_attn_a = (full_ids_a != tok.pad_token_id).long()
   396                                                       #     full_embed_a = lm(full_ids_a, attention_mask=full_attn_a).last_hidden_state.mean(dim=1)
   397                                                       #
   398                                                       #     full_ids_b = input_ids_b[i, :lb].unsqueeze(0).to(lm_device)
   399                                                       #     full_attn_b = (full_ids_b != tok.pad_token_id).long()
   400                                                       #     full_embed_b = lm(full_ids_b, attention_mask=full_attn_b).last_hidden_state.mean(dim=1)
   401                                                       #
   402                                                       #     if sentence_embeds_a.shape[0] > 0:
   403                                                       #         query_tensor = torch.cat([sentence_embeds_a, full_embed_a], dim=0)
   404                                                       #     else:
   405                                                       #         query_tensor = full_embed_a
   406                                                       #
   407                                                       #     if sentence_embeds_b.shape[0] > 0:
   408                                                       #         corpus_tensor = torch.cat([sentence_embeds_b, full_embed_b], dim=0)
   409                                                       #     else:
   410                                                       #         corpus_tensor = full_embed_b
   411                                                       # ============================================================
   412                                           
   413                                                       # FAST VERSION：直接复用 token 级嵌入做均值池化
   414        20          0.2      0.0      2.1              with torch.no_grad():
   415        10          0.2      0.0      2.1                  token_emb_a = td["token_embeddings_a"][i, :la].to(lm_device)
   416        10          0.2      0.0      1.4                  token_emb_b = td["token_embeddings_b"][i, :lb].to(lm_device)
   417                                           
   418        10          0.0      0.0      0.0                  seg_emb_a_list = []
   419        10          0.0      0.0      0.0                  prev = 0
   420        43          0.0      0.0      0.1                  for p in bounds_a:
   421        33          0.0      0.0      0.1                      end = p + 1
   422        33          0.0      0.0      0.1                      real_start = (prev + 1) if prev > 0 else 1  # skip [CLS]
   423        33          0.0      0.0      0.1                      if end > real_start:
   424        33          0.6      0.0      5.1                          seg_emb_a_list.append(token_emb_a[real_start:end].mean(dim=0))
   425        33          0.0      0.0      0.1                      prev = p
   426                                                           # Tail segment after last boundary
   427        10          0.0      0.0      0.0                  tail_start = (prev + 1) if prev > 0 else 1
   428        10          0.0      0.0      0.0                  if tail_start < la:
   429         2          0.0      0.0      0.2                      seg_emb_a_list.append(token_emb_a[tail_start:la].mean(dim=0))
   430        10          0.0      0.0      0.1                  if len(seg_emb_a_list) == 0:
   431                                                               rewards[i] = 0.0
   432                                                               continue
   433        10          0.2      0.0      1.8                  sentence_embeds_a = torch.stack(seg_emb_a_list, dim=0)
   434                                           
   435        10          0.0      0.0      0.0                  seg_emb_b_list = []
   436        10          0.0      0.0      0.0                  prev = 0
   437        41          0.0      0.0      0.1                  for p in bounds_b:
   438        31          0.0      0.0      0.1                      end = p + 1
   439        31          0.0      0.0      0.1                      real_start = (prev + 1) if prev > 0 else 1  # skip [CLS]
   440        31          0.0      0.0      0.1                      if end > real_start:
   441        31          0.5      0.0      4.0                          seg_emb_b_list.append(token_emb_b[real_start:end].mean(dim=0))
   442        31          0.0      0.0      0.1                      prev = p
   443        10          0.0      0.0      0.0                  tail_start = (prev + 1) if prev > 0 else 1
   444        10          0.0      0.0      0.0                  if tail_start < lb:
   445                                                               seg_emb_b_list.append(token_emb_b[tail_start:lb].mean(dim=0))
   446        10          0.0      0.0      0.1                  if len(seg_emb_b_list) == 0:
   447                                                               rewards[i] = 0.0
   448                                                               continue
   449        10          0.1      0.0      1.0                  sentence_embeds_b = torch.stack(seg_emb_b_list, dim=0)
   450                                           
   451        10          0.1      0.0      1.0                  full_embed_a = token_emb_a.mean(dim=0, keepdim=True)
   452        10          0.1      0.0      0.8                  full_embed_b = token_emb_b.mean(dim=0, keepdim=True)
   453                                           
   454        10          0.0      0.0      0.1                  if sentence_embeds_a.shape[0] > 0:
   455        10          0.1      0.0      1.2                      query_tensor = torch.cat([sentence_embeds_a, full_embed_a], dim=0)
   456                                                           else:
   457                                                               query_tensor = full_embed_a
   458                                           
   459        10          0.0      0.0      0.1                  if sentence_embeds_b.shape[0] > 0:
   460        10          0.1      0.0      0.8                      corpus_tensor = torch.cat([sentence_embeds_b, full_embed_b], dim=0)
   461                                                           else:
   462                                                               corpus_tensor = full_embed_b
   463                                           
   464        10          0.2      0.0      1.6                  query_weights = torch.ones(sentence_embeds_a.shape[0], device=query_tensor.device)
   465        10          0.1      0.0      0.8                  corpus_weights = torch.ones(sentence_embeds_b.shape[0], device=corpus_tensor.device)
   466                                           
   467        30          6.0      0.2     51.3                  rewards[i] = self.raw_score_text(
   468        10          0.0      0.0      0.0                      query_tensor, corpus_tensor, query_weights, corpus_weights, times=0
   469        10          0.0      0.0      0.3                  ).to(rewards.device)
   470                                           
   471        10          0.0      0.0      0.0          return rewards

Total time: 0.307945 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/MaxSimSplitter.py
Function: MaxSimSplitter.split_pair_return_segments at line 175

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   175                                               def split_pair_return_segments(self, text_a, text_b):
   176                                                   """
   177                                                   输入：Query 和 Cache Candidate
   178                                                   输出：RL模型优化后的 A片段列表 和 B片段列表
   179                                                   """
   180                                                   # 1. 构造输入 (Joint Input)
   181                                                   # Batch the two texts to avoid two separate tokenizer + LM forward passes.
   182        30         42.7      1.4     13.9          inputs = self.generator.tokenizer(
   183        10          0.0      0.0      0.0              [text_a, text_b],
   184        10          0.0      0.0      0.0              return_tensors="pt",
   185        10          0.0      0.0      0.0              padding="max_length",
   186        10          0.0      0.0      0.0              truncation=True,
   187        10          0.0      0.0      0.0              max_length=512,
   188        10          1.4      0.1      0.4          ).to(self.device)
   189                                           
   190        20          0.2      0.0      0.1          with torch.no_grad():
   191        10        128.6     12.9     41.8              hs = self.generator.lm(**inputs).last_hidden_state  # [2, L, H]
   192                                           
   193                                                   # Split batch back into the A/B shapes expected downstream (batch_size=1 each).
   194        10          0.1      0.0      0.0          embeds_a = hs[0:1, :, :]
   195        10          0.1      0.0      0.0          embeds_b = hs[1:2, :, :]
   196        10          0.1      0.0      0.0          input_ids_a = inputs["input_ids"][0:1, :]
   197        10          0.1      0.0      0.0          input_ids_b = inputs["input_ids"][1:2, :]
   198        10          0.1      0.0      0.0          attention_mask_a = inputs["attention_mask"][0:1, :]
   199        10          0.1      0.0      0.0          attention_mask_b = inputs["attention_mask"][1:2, :]
   200                                           
   201        20          1.9      0.1      0.6          td = TensorDict(
   202        10          0.0      0.0      0.0              {
   203        10          0.0      0.0      0.0                  "token_embeddings_a": embeds_a,
   204        10          0.0      0.0      0.0                  "token_embeddings_b": embeds_b,
   205        10          0.0      0.0      0.0                  "attention_mask_a": attention_mask_a,
   206        10          0.0      0.0      0.0                  "attention_mask_b": attention_mask_b,
   207        10          0.4      0.0      0.1                  "length_a": attention_mask_a.sum(dim=1),
   208        10          0.1      0.0      0.0                  "length_b": attention_mask_b.sum(dim=1),
   209        10          0.0      0.0      0.0                  "input_ids_a": input_ids_a,
   210        10          0.0      0.0      0.0                  "input_ids_b": input_ids_b,
   211                                                       },
   212        10          0.0      0.0      0.0              batch_size=1,
   213                                                   )
   214                                           
   215                                                   # 2. Greedy Decoding 获取最佳切分动作
   216        20          0.2      0.0      0.0          with torch.no_grad():
   217        10        124.4     12.4     40.4              out = self.policy(td, None, phase="test", select_best=True, decode_type="greedy")
   218                                                   
   219        10          0.1      0.0      0.0          actions = out['actions'][0] # [2 * max_segments]
   220                                           
   221                                                   # 3. 解析动作 -> 文本片段
   222                                                   # NOTE: In this repo, actions are interleaved: [A0, B0, A1, B1, ...]
   223                                                   # See `inspect_punctuation_cases.py` and `MaxSimEnv._step` (full-plan interleaved layout).
   224        10          0.0      0.0      0.0          if not isinstance(actions, torch.Tensor):
   225                                                       actions = torch.as_tensor(actions, device=self.device)
   226        10          0.0      0.0      0.0          total = int(actions.numel())
   227        10          0.0      0.0      0.0          if total % 2 != 0:
   228                                                       raise ValueError(f"Expected even number of action entries (A/B interleaved), got {total}")
   229        10          0.0      0.0      0.0          max_segments = total // 2
   230        10          0.5      0.1      0.2          pointers_a = actions[0: 2 * max_segments: 2].tolist()
   231        10          0.3      0.0      0.1          pointers_b = actions[1: 2 * max_segments: 2].tolist()
   232                                                   
   233                                                   # Reconstruct segments in **token-index space** (pointers are token positions).
   234                                                   # This avoids the previous mismatch where pointers (token indices) were applied
   235                                                   # to `prompt.lower().split()` (word indices).
   236        20          3.6      0.2      1.2          segments_a = get_segments_from_token_pointers(
   237        10          0.0      0.0      0.0              tokenizer=self.generator.tokenizer,
   238        10          0.1      0.0      0.0              input_ids=input_ids_a[0],
   239        10          0.0      0.0      0.0              attention_mask=attention_mask_a[0],
   240        10          0.0      0.0      0.0              pointers=pointers_a,
   241                                                   )
   242        20          2.6      0.1      0.8          segments_b = get_segments_from_token_pointers(
   243        10          0.0      0.0      0.0              tokenizer=self.generator.tokenizer,
   244        10          0.1      0.0      0.0              input_ids=input_ids_b[0],
   245        10          0.0      0.0      0.0              attention_mask=attention_mask_b[0],
   246        10          0.0      0.0      0.0              pointers=pointers_b,
   247                                                   )
   248                                                   
   249        10          0.0      0.0      0.0          return segments_a, segments_b

Total time: 0.137469 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/embedding_model.py
Function: EmbeddingModel.get_embedding at line 86

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    86                                               def get_embedding(self, text):
    87                                                   """ 获取文本的向量嵌入 """
    88                                                 
    89        10          0.4      0.0      0.3          device = next(self.model.parameters()).device
    90                                               
    91        10          9.1      0.9      6.6          inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    92                                                   
    93        10          2.2      0.2      1.6          inputs = {k: v.to(device) for k, v in inputs.items()}
    94                                           
    95        20          0.2      0.0      0.2          with torch.no_grad():
    96        10        123.2     12.3     89.6              outputs = self.model(**inputs)
    97                                           
    98        10          0.0      0.0      0.0          hs = outputs.last_hidden_state  # [1, L, H]
    99        10          0.0      0.0      0.0          attn = inputs.get("attention_mask", None)
   100        10          0.0      0.0      0.0          if attn is None:
   101                                                       pooled = hs.mean(dim=1)
   102                                                   else:
   103        10          0.4      0.0      0.3              attn_f = attn.to(dtype=hs.dtype).unsqueeze(-1)
   104        10          1.2      0.1      0.9              pooled = (hs * attn_f).sum(dim=1) / attn_f.sum(dim=1).clamp_min(1.0)
   105                                           
   106        10          0.8      0.1      0.6          return pooled.squeeze().cpu().numpy()

Total time: 0.144284 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/embedding_model.py
Function: EmbeddingModel.get_embeddings_tensor at line 146

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   146                                               def get_embeddings_tensor(
   147                                                   self, texts: list[str], device: torch.device | str | None = None
   148                                               ) -> torch.Tensor:
   149                                                   """
   150                                                   Batched version of `get_embedding_tensor`.
   151                                           
   152                                                   Semantics: matches what you'd get by calling `get_embedding_tensor` on each
   153                                                   text independently (batch_size=1 => no padding) by using **masked mean pooling**
   154                                                   over non-padding tokens when batching introduces padding.
   155                                           
   156                                                   Returns:
   157                                                       torch.Tensor of shape [len(texts), hidden_size] on `device`.
   158                                                   """
   159        10          0.0      0.0      0.0          if not texts:
   160                                                       raise ValueError("texts must be a non-empty list[str]")
   161                                           
   162        10          0.0      0.0      0.0          if device is None:
   163                                                       device = next(self.model.parameters()).device
   164                                                   else:
   165        10          0.0      0.0      0.0              device = torch.device(device)
   166                                           
   167                                                   # Ensure model is on the target device.
   168                                                   #
   169                                                   # Important: `torch.device("cuda")` has `index=None`, while a model is typically
   170                                                   # on `cuda:0`. Treat these as equivalent to avoid an expensive `.to(...)` call
   171                                                   # on every invocation in hot paths.
   172        10          0.2      0.0      0.2          cur = next(self.model.parameters()).device
   173        10          0.0      0.0      0.0          same_device = (cur == device)
   174        10          0.0      0.0      0.0          if not same_device and cur.type == "cuda" and device.type == "cuda":
   175                                                       # If caller asked for generic "cuda", accept the current cuda:<idx>.
   176                                                       if device.index is None:
   177                                                           same_device = True
   178                                                       # If caller asked for a specific cuda:<idx>, require exact match.
   179        10          0.0      0.0      0.0          if not same_device:
   180                                                       self.model.to(device)
   181                                           
   182        10         44.1      4.4     30.6          inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
   183        10          1.3      0.1      0.9          inputs = {k: v.to(device) for k, v in inputs.items()}
   184                                           
   185        20          0.2      0.0      0.2          with torch.no_grad():
   186        10         97.2      9.7     67.4              outputs = self.model(**inputs)
   187                                           
   188        10          0.0      0.0      0.0          hs = outputs.last_hidden_state  # [B, L, H]
   189        10          0.0      0.0      0.0          attn = inputs.get("attention_mask", None)
   190        10          0.0      0.0      0.0          if attn is None:
   191                                                       # Fallback to unmasked mean (shouldn't happen for HF tokenizers)
   192                                                       return hs.mean(dim=1)
   193                                           
   194                                                   # Masked mean pooling: sum over real tokens / count(real tokens)
   195                                                   # This reproduces the batch_size=1 behavior (no pad tokens).
   196        10          0.3      0.0      0.2          attn_f = attn.to(dtype=hs.dtype).unsqueeze(-1)  # [B, L, 1]
   197        10          0.4      0.0      0.3          summed = (hs * attn_f).sum(dim=1)  # [B, H]
   198        10          0.3      0.0      0.2          counts = attn_f.sum(dim=1).clamp_min(1.0)  # [B, 1]
   199        10          0.2      0.0      0.1          return summed / counts

Total time: 0.476653 s
File: /home/ali/vcahce/vcache/vcache_policy/strategies/verified_splitter.py
Function: VerifiedSplitterDecisionPolicy._maxsim_similarity at line 113

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   113                                               def _maxsim_similarity(self, query: str, candidate: str) -> float:
   114                                                   """
   115                                                   Compute MaxSim similarity in [0, 1] using the provided splitter + its embedding model.
   116                                                   """
   117        10          0.0      0.0      0.0          if self.splitter is None:
   118                                                       raise ValueError(
   119                                                           "VerifiedSplitterDecisionPolicy requires `splitter` (MaxSimSplitter) to be provided."
   120                                                       )
   121                                           
   122        10          0.0      0.0      0.0          try:
   123        10          0.0      0.0      0.0              import torch
   124        10          0.0      0.0      0.0              import torch.nn.functional as F
   125                                                   except Exception as e:
   126                                                       raise RuntimeError(
   127                                                           "VerifiedSplitterDecisionPolicy requires torch to compute MaxSim similarity."
   128                                                       ) from e
   129                                           
   130        10        308.7     30.9     64.8          segments_a, segments_b = self.splitter.split_pair_return_segments(query, candidate)
   131                                           
   132                                                   # Keep embeddings + similarity math on the policy device to avoid GPU->CPU->GPU round trips.
   133                                                   # NOTE: We preserve the exact per-text behavior of `get_embedding_tensor` (batch_size=1 => no pad)
   134                                                   # while batching by using masked-mean pooling in `get_embeddings_tensor`.
   135        10          0.0      0.0      0.0          emb = self.splitter.embedding_model
   136                                                   # Use the embedding model's *actual* parameter device to avoid cuda vs cuda:0 mismatches
   137                                                   # that would trigger expensive `model.to(device)` moves in hot loops.
   138        10          0.0      0.0      0.0          try:
   139        10          0.3      0.0      0.1              dev = next(emb.model.parameters()).device
   140                                                   except Exception:
   141                                                       dev = torch.device(self.device) if not isinstance(self.device, torch.device) else self.device
   142                                           
   143                                                   # Batch embeddings: do a *single* model forward for (segments_a + query + segments_b + candidate),
   144                                                   # then split back into A/B slices. This reduces tokenizer overhead and avoids a second forward.
   145        10          0.0      0.0      0.0          a_texts = list(segments_a) + [query]
   146        10          0.0      0.0      0.0          b_texts = list(segments_b) + [candidate]
   147        10          0.0      0.0      0.0          all_texts = a_texts + b_texts
   148        10        144.5     14.5     30.3          all_emb = emb.get_embeddings_tensor(all_texts, device=dev).to(dtype=torch.float32)  # [Sa+Sb+2, D]
   149        10          0.0      0.0      0.0          a_n = len(a_texts)
   150        10          0.1      0.0      0.0          a_emb = all_emb[:a_n, :]
   151        10          0.1      0.0      0.0          b_emb = all_emb[a_n:, :]
   152                                           
   153        10          0.1      0.0      0.0          seg_a_t = a_emb[:-1, :] if a_emb.shape[0] > 1 else a_emb[:0, :]
   154        10          0.1      0.0      0.0          full_a_t = a_emb[-1:, :]
   155        10          0.1      0.0      0.0          seg_b_t = b_emb[:-1, :] if b_emb.shape[0] > 1 else b_emb[:0, :]
   156        10          0.1      0.0      0.0          full_b_t = b_emb[-1:, :]
   157                                           
   158                                                   # query_tensor/corpus_tensor follow MaxSimEnv.raw_score_text convention:
   159                                                   # [sentence_embeds..., full_embed]
   160        10          0.2      0.0      0.0          query_tensor = torch.cat([seg_a_t, full_a_t], dim=0)
   161        10          0.1      0.0      0.0          corpus_tensor = torch.cat([seg_b_t, full_b_t], dim=0)
   162                                           
   163                                                   # Weights mimic MaxSimEnv: score_weights_raw = [-1e9, 0, 0] => coarse ~0, row/col ~0.5 each
   164        10         17.1      1.7      3.6          weights = torch.softmax(torch.tensor([-1e9, 0.0, 0.0], device=dev, dtype=torch.float32), dim=0)
   165        10          0.2      0.0      0.1          w_coarse, w_row, w_col = weights.tolist()
   166                                           
   167        10          1.3      0.1      0.3          coarse = F.cosine_similarity(query_tensor[-1:, :], corpus_tensor[-1:, :]).squeeze()
   168                                           
   169        10          0.1      0.0      0.0          query_sentence = query_tensor[:-1, :]
   170        10          0.1      0.0      0.0          corpus_sentence = corpus_tensor[:-1, :]
   171        10          0.0      0.0      0.0          if query_sentence.shape[0] > 0 and corpus_sentence.shape[0] > 0:
   172        10          0.7      0.1      0.2              qn = F.normalize(query_sentence, p=2, dim=-1)
   173        10          0.4      0.0      0.1              cn = F.normalize(corpus_sentence, p=2, dim=-1)
   174        10          0.4      0.0      0.1              cos = torch.mm(qn, cn.T)
   175        10          0.4      0.0      0.1              row_score = torch.max(cos, dim=1).values.mean()
   176        10          0.3      0.0      0.1              col_score = torch.max(cos, dim=0).values.mean()
   177                                                   else:
   178                                                       row_score = torch.tensor(0.0)
   179                                                       col_score = torch.tensor(0.0)
   180                                           
   181        10          0.5      0.1      0.1          raw = (w_coarse * coarse) + (w_row * row_score) + (w_col * col_score)
   182                                           
   183                                                   # Map [-1, 1] -> [0, 1] and clip
   184        10          0.6      0.1      0.1          s01 = float(torch.clamp((raw + 1.0) / 2.0, 0.0, 1.0).item())
   185        10          0.0      0.0      0.0          return s01

Total time: 0.616808 s
File: /home/ali/vcahce/vcache/vcache_policy/strategies/verified_splitter.py
Function: VerifiedSplitterDecisionPolicy._select_nn_by_maxsim at line 187

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   187                                               def _select_nn_by_maxsim(
   188                                                   self, prompt: str
   189                                               ) -> Tuple[Optional[EmbeddingMetadataObj], Optional[float]]:
   190                                                   """
   191                                                   Select the nearest-neighbor metadata object using MaxSim similarity.
   192                                           
   193                                                   Returns:
   194                                                       (best_metadata, best_similarity) where similarity is in [0, 1].
   195                                                   """
   196        10          0.0      0.0      0.0          if self.cache is None:
   197                                                       return None, None
   198                                           
   199        10          0.0      0.0      0.0          candidates: list[EmbeddingMetadataObj] = []
   200                                           
   201        10          0.0      0.0      0.0          if self.candidate_selection == "all":
   202                                                       candidates = self.cache.get_all_embedding_metadata_objects()
   203        10          0.0      0.0      0.0          elif self.candidate_selection == "top_k":
   204        10        139.6     14.0     22.6              knn = self.cache.get_knn(prompt=prompt, k=max(1, int(self.candidate_k)))
   205        20          0.0      0.0      0.0              for _db_score, embedding_id in knn:
   206        10          0.0      0.0      0.0                  try:
   207        10          0.1      0.0      0.0                      candidates.append(self.cache.get_metadata(embedding_id=embedding_id))
   208                                                           except Exception:
   209                                                               continue
   210                                                   else:
   211                                                       raise ValueError(
   212                                                           f"Unknown candidate_selection={self.candidate_selection!r}. Use 'top_k' or 'all'."
   213                                                       )
   214                                           
   215        10          0.0      0.0      0.0          if not candidates:
   216                                                       return None, None
   217                                           
   218        10          0.0      0.0      0.0          best_meta: Optional[EmbeddingMetadataObj] = None
   219        10          0.0      0.0      0.0          best_s: float = -1.0
   220                                           
   221        20          0.0      0.0      0.0          for meta in candidates:
   222        10          0.0      0.0      0.0              cached_prompt = getattr(meta, "prompt", "") or ""
   223        10          0.0      0.0      0.0              if not cached_prompt:
   224                                                           # Can't compute MaxSim without cached prompt text; skip.
   225                                                           continue
   226        10          0.0      0.0      0.0              try:
   227        10        477.0     47.7     77.3                  s = self._maxsim_similarity(prompt, cached_prompt)
   228                                                       except Exception as e:
   229                                                           self.logger.warning(f"MaxSim similarity failed for one candidate: {e}")
   230                                                           continue
   231        10          0.0      0.0      0.0              if s > best_s:
   232        10          0.0      0.0      0.0                  best_s = s
   233        10          0.0      0.0      0.0                  best_meta = meta
   234                                           
   235        10          0.0      0.0      0.0          if best_meta is None:
   236                                                       return None, None
   237        10          0.0      0.0      0.0          return best_meta, best_s

Total time: 0.617976 s
File: /home/ali/vcahce/vcache/vcache_policy/strategies/verified_splitter.py
Function: VerifiedSplitterDecisionPolicy.process_request at line 239

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   239                                               def process_request(
   240                                                   self, prompt: str, system_prompt: Optional[str], id_set: int
   241                                               ) -> Tuple[bool, str, EmbeddingMetadataObj]:
   242        10          0.0      0.0      0.0          if self.inference_engine is None or self.cache is None:
   243                                                       raise ValueError("Policy has not been setup")
   244                                           
   245                                                   # If cache is empty, this will return (None, None)
   246        10        617.0     61.7     99.8          nn_metadata, similarity_score = self._select_nn_by_maxsim(prompt)
   247        10          0.0      0.0      0.0          if nn_metadata is None or similarity_score is None:
   248                                                       response = self.inference_engine.create(prompt=prompt, system_prompt=system_prompt)
   249                                                       self.cache.add(prompt=prompt, response=response, id_set=id_set)
   250                                                       return False, response, EmbeddingMetadataObj(embedding_id=-1, response="")
   251                                           
   252        10          0.4      0.0      0.1          action = self.bayesian.select_action(similarity_score=similarity_score, metadata=nn_metadata)
   253                                           
   254        10          0.0      0.0      0.0          match action:
   255        10          0.0      0.0      0.0              case _Action.EXPLOIT:
   256                                                           return True, nn_metadata.response, nn_metadata
   257        10          0.0      0.0      0.0              case _Action.EXPLORE:
   258        10          0.0      0.0      0.0                  response = self.inference_engine.create(prompt=prompt, system_prompt=system_prompt)
   259        20          0.6      0.0      0.1                  self.__update_cache(
   260        10          0.0      0.0      0.0                      response=response,
   261        10          0.0      0.0      0.0                      nn_metadata=nn_metadata,
   262        10          0.0      0.0      0.0                      similarity_score=similarity_score,
   263        10          0.0      0.0      0.0                      embedding_id=nn_metadata.embedding_id,
   264        10          0.0      0.0      0.0                      prompt=prompt,
   265        10          0.0      0.0      0.0                      label_id_set=id_set,
   266                                                           )
   267        10          0.0      0.0      0.0                  return False, response, nn_metadata

