args={'dataset': 'vCache/SemBenchmarkClassification', 'llm_col': 'response_llama_3_8b', 'splitter_checkpoint': '/data2/ali/checkpoints_words', 'splitter_device': 'cuda', 'delta': 0.02, 'candidate_k': 1, 'max_samples': 12, 'warmup_samples': 2, 'profile_samples': 3, 'sleep': 0.0, 'hf_cache_base': '/data2/ali/hf', 'output': 'results/line_profile_verified_splitter_cuda.txt'}
python=3.11.14 (main, Oct 21 2025, 18:31:21) [GCC 11.2.0]
cwd=/home/ali/vcahce

Timer unit: 0.001 s

Total time: 0.581306 s
File: /home/ali/vcahce/benchmarks/line_profile_verified_splitter.py
Function: main.<locals>.<lambda> at line 224

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   224         2        581.3    290.7    100.0          lambda: _run_loop(
   225         1          0.0      0.0      0.0              vcache=vcache,
   226         1          0.0      0.0      0.0              rows=rows_list,
   227         1          0.0      0.0      0.0              llm_col=args.llm_col,
   228         1          0.0      0.0      0.0              warmup_samples=int(args.warmup_samples),
   229         1          0.0      0.0      0.0              profile_samples=int(args.profile_samples),
   230         1          0.0      0.0      0.0              sleep_s=float(args.sleep),
   231                                                   )

Total time: 0.581183 s
File: /home/ali/vcahce/vcache/main.py
Function: VCache.infer_with_cache_info at line 53

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    53                                               def infer_with_cache_info(
    54                                                   self,
    55                                                   prompt: str,
    56                                                   system_prompt: Optional[str] = None,
    57                                                   id_set: int = -1,
    58                                               ) -> Tuple[bool, str, EmbeddingMetadataObj, EmbeddingMetadataObj]:
    59                                                   """Infers a response and returns the cache hit status and metadata.
    60                                           
    61                                                   Args:
    62                                                       prompt (str): The prompt to create a response for.
    63                                                       system_prompt (Optional[str]): The optional system prompt to use
    64                                                           for the response. Overrides the system prompt in the
    65                                                           VCacheConfig if provided.
    66                                                       id_set (int): The set identifier for the embedding. This is used in the
    67                                                           benchmark to identify if the nearest neighbor is from the same set
    68                                                           (if the cached response is correct or incorrect).
    69                                           
    70                                                   Returns:
    71                                                       Tuple[bool, str, EmbeddingMetadataObj, EmbeddingMetadataObj]: A tuple containing the cache
    72                                                           hit status, the response, the metadata of the response and nearest neighbor metadata.
    73                                                   """
    74         5          0.0      0.0      0.0          if system_prompt is None:
    75                                                       system_prompt = self.vcache_config.system_prompt
    76                                           
    77         5          0.0      0.0      0.0          if self.vcache_config.eviction_policy.is_evicting():
    78                                                       response = self.__generate_response(prompt, system_prompt)
    79                                                       return (
    80                                                           False,
    81                                                           response,
    82                                                           EmbeddingMetadataObj(embedding_id=-1, response=response, id_set=id_set),
    83                                                           EmbeddingMetadataObj(embedding_id=-1, response=response, id_set=id_set),
    84                                                       )
    85                                           
    86        10        579.9     58.0     99.8          is_cache_hit, response, nn_metadata = self.vcache_policy.process_request(
    87         5          0.0      0.0      0.0              prompt, system_prompt, id_set
    88                                                   )
    89                                           
    90         5          0.0      0.0      0.0          if nn_metadata is not None:
    91         5          0.0      0.0      0.0              self.vcache_config.eviction_policy.update_eviction_metadata(nn_metadata)
    92                                           
    93         5          0.0      0.0      0.0          nn_metadata_copy: Optional[EmbeddingMetadataObj] = (
    94         5          1.2      0.2      0.2              copy.deepcopy(nn_metadata) if nn_metadata is not None else None
    95                                                   )
    96                                           
    97         5          0.0      0.0      0.0          if self.vcache_config.eviction_policy.ready_to_evict(self.vcache_policy.cache):
    98                                                       self.vcache_config.eviction_policy.evict(self.vcache_policy.cache)
    99                                           
   100         5          0.0      0.0      0.0          if is_cache_hit:
   101                                                       return is_cache_hit, response, nn_metadata_copy, nn_metadata_copy
   102                                                   else:
   103         5          0.0      0.0      0.0              return (
   104         5          0.0      0.0      0.0                  is_cache_hit,
   105         5          0.0      0.0      0.0                  response,
   106         5          0.1      0.0      0.0                  EmbeddingMetadataObj(embedding_id=-1, response=response, id_set=id_set),
   107         5          0.0      0.0      0.0                  nn_metadata_copy,
   108                                                       )

Total time: 0.132373 s
File: /home/ali/vcahce/vcache/vcache_core/cache/cache.py
Function: Cache.get_knn at line 65

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    65                                               def get_knn(self, prompt: str, k: int) -> List[tuple[float, int]]:
    66                                                   """Gets k-nearest neighbors for a given prompt.
    67                                           
    68                                                   Args:
    69                                                       prompt (str): The prompt to get the k-nearest neighbors for.
    70                                                       k (int): The number of nearest neighbors to retrieve.
    71                                           
    72                                                   Returns:
    73                                                       List[tuple[float, int]]: A list of tuples, each containing a
    74                                                       similarity score and an embedding ID.
    75                                                   """
    76         5        131.9     26.4     99.6          embedding = self.embedding_engine.get_embedding(prompt)
    77         5          0.5      0.1      0.4          return self.embedding_store.get_knn(embedding, k)

Total time: 0.141821 s
File: /home/ali/vcahce/vcache/vcache_core/cache/embedding_engine/strategies/bge.py
Function: BGEEmbeddingEngine.get_embedding at line 16

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    16                                               @override
    17                                               def get_embedding(self, text: str) -> List[float]:
    18                                                   """
    19                                                   Get the embedding for the given text using the underlying BGE model.
    20                                           
    21                                                   Args:
    22                                                       text: The text to get the embedding for.
    23                                           
    24                                                   Returns:
    25                                                       The embedding of the text as a list of floats.
    26                                                   """
    27                                                   # EmbeddingModel.get_embedding returns a numpy array, we convert to list
    28         6        141.7     23.6     99.9          embedding = self.embedding_model.get_embedding(text)
    29         6          0.1      0.0      0.1          return embedding.tolist()

Total time: 0.000452084 s
File: /home/ali/vcahce/vcache/vcache_core/cache/embedding_store/vector_db/strategies/hnsw_lib.py
Function: HNSWLibVectorDB.get_knn at line 80

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    80                                               def get_knn(self, embedding: List[float], k: int) -> List[tuple[float, int]]:
    81                                                   """Gets k-nearest neighbors for a given embedding.
    82                                           
    83                                                   Args:
    84                                                       embedding (List[float]): The query embedding vector.
    85                                                       k (int): The number of nearest neighbors to return.
    86                                           
    87                                                   Returns:
    88                                                       List[tuple[float, int]]: A list of tuples containing similarity
    89                                                       scores and embedding IDs.
    90                                                   """
    91         5          0.0      0.0      1.6          if self.index is None:
    92         1          0.0      0.0      0.1              return []
    93         4          0.0      0.0      1.8          k_ = min(k, self.embedding_count)
    94         4          0.0      0.0      0.6          if k_ == 0:
    95                                                       return []
    96         4          0.2      0.1     47.1          ids, similarities = self.index.knn_query(embedding, k=k_)
    97         4          0.0      0.0      5.4          metric_type = self.similarity_metric_type.value
    98         8          0.2      0.0     34.4          similarity_scores = [
    99         4          0.0      0.0      2.1              self.transform_similarity_score(sim, metric_type) for sim in similarities[0]
   100                                                   ]
   101         4          0.0      0.0      4.5          id_list = [int(id) for id in ids[0]]
   102         4          0.0      0.0      2.3          return list(zip(similarity_scores, id_list))

Total time: 0.333514 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/MaxSimSplitter.py
Function: MaxSimSplitter.split_pair_return_segments at line 175

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   175                                               def split_pair_return_segments(self, text_a, text_b):
   176                                                   """
   177                                                   输入：Query 和 Cache Candidate
   178                                                   输出：RL模型优化后的 A片段列表 和 B片段列表
   179                                                   """
   180                                                   # 1. 构造输入 (Joint Input)
   181         4          8.2      2.0      2.5          inputs_a = self.generator.tokenizer([text_a], return_tensors='pt', padding='max_length', truncation=True, max_length=512).to(self.device)
   182         4          7.6      1.9      2.3          inputs_b = self.generator.tokenizer([text_b], return_tensors='pt', padding='max_length', truncation=True, max_length=512).to(self.device)
   183                                                   
   184         8          0.1      0.0      0.0          with torch.no_grad():
   185         4         48.1     12.0     14.4              embeds_a = self.generator.lm(**inputs_a).last_hidden_state
   186         4         34.1      8.5     10.2              embeds_b = self.generator.lm(**inputs_b).last_hidden_state
   187                                                   
   188        12          0.9      0.1      0.3          td = TensorDict({
   189         4          0.0      0.0      0.0              "token_embeddings_a": embeds_a,
   190         4          0.0      0.0      0.0              "token_embeddings_b": embeds_b,
   191         4          0.0      0.0      0.0              "attention_mask_a": inputs_a['attention_mask'],
   192         4          0.0      0.0      0.0              "attention_mask_b": inputs_b['attention_mask'],
   193         4          0.1      0.0      0.0              "length_a": inputs_a['attention_mask'].sum(dim=1),
   194         4          0.0      0.0      0.0              "length_b": inputs_b['attention_mask'].sum(dim=1),
   195         4          0.0      0.0      0.0              "input_ids_a": inputs_a['input_ids'],
   196         4          0.0      0.0      0.0              "input_ids_b": inputs_b['input_ids'],
   197         4          0.0      0.0      0.0          }, batch_size=1)
   198                                           
   199                                                   # 2. Greedy Decoding 获取最佳切分动作
   200         8          0.1      0.0      0.0          with torch.no_grad():
   201         4        230.9     57.7     69.2              out = self.policy(td, None, phase="test", select_best=True, decode_type="greedy")
   202                                                   
   203         4          0.0      0.0      0.0          actions = out['actions'][0] # [2 * max_segments]
   204                                           
   205                                                   # 3. 解析动作 -> 文本片段
   206                                                   # NOTE: In this repo, actions are interleaved: [A0, B0, A1, B1, ...]
   207                                                   # See `inspect_punctuation_cases.py` and `MaxSimEnv._step` (full-plan interleaved layout).
   208         4          0.0      0.0      0.0          if not isinstance(actions, torch.Tensor):
   209                                                       actions = torch.as_tensor(actions, device=self.device)
   210         4          0.0      0.0      0.0          total = int(actions.numel())
   211         4          0.0      0.0      0.0          if total % 2 != 0:
   212                                                       raise ValueError(f"Expected even number of action entries (A/B interleaved), got {total}")
   213         4          0.0      0.0      0.0          max_segments = total // 2
   214         4          0.2      0.1      0.1          pointers_a = actions[0: 2 * max_segments: 2].tolist()
   215         4          0.1      0.0      0.0          pointers_b = actions[1: 2 * max_segments: 2].tolist()
   216                                                   
   217                                                   # Reconstruct segments in **token-index space** (pointers are token positions).
   218                                                   # This avoids the previous mismatch where pointers (token indices) were applied
   219                                                   # to `prompt.lower().split()` (word indices).
   220         8          1.6      0.2      0.5          segments_a = get_segments_from_token_pointers(
   221         4          0.0      0.0      0.0              tokenizer=self.generator.tokenizer,
   222         4          0.0      0.0      0.0              input_ids=inputs_a["input_ids"][0],
   223         4          0.0      0.0      0.0              attention_mask=inputs_a.get("attention_mask", None)[0]
   224         4          0.0      0.0      0.0              if inputs_a.get("attention_mask", None) is not None
   225                                                       else None,
   226         4          0.0      0.0      0.0              pointers=pointers_a,
   227                                                   )
   228         8          1.2      0.2      0.4          segments_b = get_segments_from_token_pointers(
   229         4          0.0      0.0      0.0              tokenizer=self.generator.tokenizer,
   230         4          0.0      0.0      0.0              input_ids=inputs_b["input_ids"][0],
   231         4          0.0      0.0      0.0              attention_mask=inputs_b.get("attention_mask", None)[0]
   232         4          0.0      0.0      0.0              if inputs_b.get("attention_mask", None) is not None
   233                                                       else None,
   234         4          0.0      0.0      0.0              pointers=pointers_b,
   235                                                   )
   236                                                   
   237         4          0.0      0.0      0.0          return segments_a, segments_b

Total time: 0.14154 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/embedding_model.py
Function: EmbeddingModel.get_embedding at line 86

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    86                                               def get_embedding(self, text):
    87                                                   """ 获取文本的向量嵌入 """
    88                                                 
    89         6          0.3      0.0      0.2          device = next(self.model.parameters()).device
    90                                               
    91         6          6.4      1.1      4.5          inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    92                                                   
    93         6          1.3      0.2      0.9          inputs = {k: v.to(device) for k, v in inputs.items()}
    94                                           
    95        12          0.2      0.0      0.1          with torch.no_grad():
    96         6        109.9     18.3     77.7              outputs = self.model(**inputs)
    97                                           
    98         6          0.0      0.0      0.0          hs = outputs.last_hidden_state  # [1, L, H]
    99         6          0.0      0.0      0.0          attn = inputs.get("attention_mask", None)
   100         6          0.0      0.0      0.0          if attn is None:
   101                                                       pooled = hs.mean(dim=1)
   102                                                   else:
   103         6          0.3      0.1      0.2              attn_f = attn.to(dtype=hs.dtype).unsqueeze(-1)
   104         6         22.7      3.8     16.0              pooled = (hs * attn_f).sum(dim=1) / attn_f.sum(dim=1).clamp_min(1.0)
   105                                           
   106         6          0.5      0.1      0.4          return pooled.squeeze().cpu().numpy()

Total time: 0.0979934 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/embedding_model.py
Function: EmbeddingModel.get_embeddings_tensor at line 146

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   146                                               def get_embeddings_tensor(
   147                                                   self, texts: list[str], device: torch.device | str | None = None
   148                                               ) -> torch.Tensor:
   149                                                   """
   150                                                   Batched version of `get_embedding_tensor`.
   151                                           
   152                                                   Semantics: matches what you'd get by calling `get_embedding_tensor` on each
   153                                                   text independently (batch_size=1 => no padding) by using **masked mean pooling**
   154                                                   over non-padding tokens when batching introduces padding.
   155                                           
   156                                                   Returns:
   157                                                       torch.Tensor of shape [len(texts), hidden_size] on `device`.
   158                                                   """
   159         8          0.0      0.0      0.0          if not texts:
   160                                                       raise ValueError("texts must be a non-empty list[str]")
   161                                           
   162         8          0.0      0.0      0.0          if device is None:
   163                                                       device = next(self.model.parameters()).device
   164                                                   else:
   165         8          0.0      0.0      0.0              device = torch.device(device)
   166                                           
   167                                                   # Ensure model is on the target device.
   168                                                   #
   169                                                   # Important: `torch.device("cuda")` has `index=None`, while a model is typically
   170                                                   # on `cuda:0`. Treat these as equivalent to avoid an expensive `.to(...)` call
   171                                                   # on every invocation in hot paths.
   172         8          0.2      0.0      0.3          cur = next(self.model.parameters()).device
   173         8          0.0      0.0      0.0          same_device = (cur == device)
   174         8          0.0      0.0      0.0          if not same_device and cur.type == "cuda" and device.type == "cuda":
   175                                                       # If caller asked for generic "cuda", accept the current cuda:<idx>.
   176                                                       if device.index is None:
   177                                                           same_device = True
   178                                                       # If caller asked for a specific cuda:<idx>, require exact match.
   179         8          0.0      0.0      0.0          if not same_device:
   180                                                       self.model.to(device)
   181                                           
   182         8         22.0      2.8     22.5          inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
   183         8          0.9      0.1      0.9          inputs = {k: v.to(device) for k, v in inputs.items()}
   184                                           
   185        16          0.2      0.0      0.2          with torch.no_grad():
   186         8         73.8      9.2     75.3              outputs = self.model(**inputs)
   187                                           
   188         8          0.0      0.0      0.0          hs = outputs.last_hidden_state  # [B, L, H]
   189         8          0.0      0.0      0.0          attn = inputs.get("attention_mask", None)
   190         8          0.0      0.0      0.0          if attn is None:
   191                                                       # Fallback to unmasked mean (shouldn't happen for HF tokenizers)
   192                                                       return hs.mean(dim=1)
   193                                           
   194                                                   # Masked mean pooling: sum over real tokens / count(real tokens)
   195                                                   # This reproduces the batch_size=1 behavior (no pad tokens).
   196         8          0.2      0.0      0.2          attn_f = attn.to(dtype=hs.dtype).unsqueeze(-1)  # [B, L, 1]
   197         8          0.3      0.0      0.3          summed = (hs * attn_f).sum(dim=1)  # [B, H]
   198         8          0.2      0.0      0.2          counts = attn_f.sum(dim=1).clamp_min(1.0)  # [B, 1]
   199         8          0.1      0.0      0.1          return summed / counts

Total time: 0.435217 s
File: /home/ali/vcahce/vcache/vcache_policy/strategies/verified_splitter.py
Function: VerifiedSplitterDecisionPolicy._maxsim_similarity at line 113

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   113                                               def _maxsim_similarity(self, query: str, candidate: str) -> float:
   114                                                   """
   115                                                   Compute MaxSim similarity in [0, 1] using the provided splitter + its embedding model.
   116                                                   """
   117         4          0.0      0.0      0.0          if self.splitter is None:
   118                                                       raise ValueError(
   119                                                           "VerifiedSplitterDecisionPolicy requires `splitter` (MaxSimSplitter) to be provided."
   120                                                       )
   121                                           
   122         4          0.0      0.0      0.0          try:
   123         4          0.0      0.0      0.0              import torch
   124         4          0.0      0.0      0.0              import torch.nn.functional as F
   125                                                   except Exception as e:
   126                                                       raise RuntimeError(
   127                                                           "VerifiedSplitterDecisionPolicy requires torch to compute MaxSim similarity."
   128                                                       ) from e
   129                                           
   130         4        333.9     83.5     76.7          segments_a, segments_b = self.splitter.split_pair_return_segments(query, candidate)
   131                                           
   132                                                   # Keep embeddings + similarity math on the policy device to avoid GPU->CPU->GPU round trips.
   133                                                   # NOTE: We preserve the exact per-text behavior of `get_embedding_tensor` (batch_size=1 => no pad)
   134                                                   # while batching by using masked-mean pooling in `get_embeddings_tensor`.
   135         4          0.0      0.0      0.0          emb = self.splitter.embedding_model
   136                                                   # Use the embedding model's *actual* parameter device to avoid cuda vs cuda:0 mismatches
   137                                                   # that would trigger expensive `model.to(device)` moves in hot loops.
   138         4          0.0      0.0      0.0          try:
   139         4          0.2      0.0      0.0              dev = next(emb.model.parameters()).device
   140                                                   except Exception:
   141                                                       dev = torch.device(self.device) if not isinstance(self.device, torch.device) else self.device
   142                                           
   143                                                   # Batch embeddings: 2 model forwards instead of (len(segments_a)+len(segments_b)+2) forwards.
   144         4          0.0      0.0      0.0          a_texts = list(segments_a) + [query]
   145         4          0.0      0.0      0.0          b_texts = list(segments_b) + [candidate]
   146         4         51.6     12.9     11.9          a_emb = emb.get_embeddings_tensor(a_texts, device=dev).to(dtype=torch.float32)  # [Sa+1, D]
   147         4         46.6     11.6     10.7          b_emb = emb.get_embeddings_tensor(b_texts, device=dev).to(dtype=torch.float32)  # [Sb+1, D]
   148                                           
   149         4          0.1      0.0      0.0          seg_a_t = a_emb[:-1, :] if a_emb.shape[0] > 1 else a_emb[:0, :]
   150         4          0.0      0.0      0.0          full_a_t = a_emb[-1:, :]
   151         4          0.0      0.0      0.0          seg_b_t = b_emb[:-1, :] if b_emb.shape[0] > 1 else b_emb[:0, :]
   152         4          0.0      0.0      0.0          full_b_t = b_emb[-1:, :]
   153                                           
   154                                                   # query_tensor/corpus_tensor follow MaxSimEnv.raw_score_text convention:
   155                                                   # [sentence_embeds..., full_embed]
   156         4          0.1      0.0      0.0          query_tensor = torch.cat([seg_a_t, full_a_t], dim=0)
   157         4          0.0      0.0      0.0          corpus_tensor = torch.cat([seg_b_t, full_b_t], dim=0)
   158                                           
   159                                                   # Weights mimic MaxSimEnv: score_weights_raw = [-1e9, 0, 0] => coarse ~0, row/col ~0.5 each
   160         4          0.2      0.1      0.1          weights = torch.softmax(torch.tensor([-1e9, 0.0, 0.0], device=dev, dtype=torch.float32), dim=0)
   161         4          0.1      0.0      0.0          w_coarse, w_row, w_col = weights.tolist()
   162                                           
   163         4          0.5      0.1      0.1          coarse = F.cosine_similarity(query_tensor[-1:, :], corpus_tensor[-1:, :]).squeeze()
   164                                           
   165         4          0.0      0.0      0.0          query_sentence = query_tensor[:-1, :]
   166         4          0.0      0.0      0.0          corpus_sentence = corpus_tensor[:-1, :]
   167         4          0.0      0.0      0.0          if query_sentence.shape[0] > 0 and corpus_sentence.shape[0] > 0:
   168         4          0.3      0.1      0.1              qn = F.normalize(query_sentence, p=2, dim=-1)
   169         4          0.2      0.0      0.0              cn = F.normalize(corpus_sentence, p=2, dim=-1)
   170         4          0.4      0.1      0.1              cos = torch.mm(qn, cn.T)
   171         4          0.3      0.1      0.1              row_score = torch.max(cos, dim=1).values.mean()
   172         4          0.1      0.0      0.0              col_score = torch.max(cos, dim=0).values.mean()
   173                                                   else:
   174                                                       row_score = torch.tensor(0.0)
   175                                                       col_score = torch.tensor(0.0)
   176                                           
   177         4          0.2      0.1      0.0          raw = (w_coarse * coarse) + (w_row * row_score) + (w_col * col_score)
   178                                           
   179                                                   # Map [-1, 1] -> [0, 1] and clip
   180         4          0.3      0.1      0.1          s01 = float(torch.clamp((raw + 1.0) / 2.0, 0.0, 1.0).item())
   181         4          0.0      0.0      0.0          return s01

Total time: 0.567856 s
File: /home/ali/vcahce/vcache/vcache_policy/strategies/verified_splitter.py
Function: VerifiedSplitterDecisionPolicy._select_nn_by_maxsim at line 183

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   183                                               def _select_nn_by_maxsim(
   184                                                   self, prompt: str
   185                                               ) -> Tuple[Optional[EmbeddingMetadataObj], Optional[float]]:
   186                                                   """
   187                                                   Select the nearest-neighbor metadata object using MaxSim similarity.
   188                                           
   189                                                   Returns:
   190                                                       (best_metadata, best_similarity) where similarity is in [0, 1].
   191                                                   """
   192         5          0.0      0.0      0.0          if self.cache is None:
   193                                                       return None, None
   194                                           
   195         5          0.0      0.0      0.0          candidates: list[EmbeddingMetadataObj] = []
   196                                           
   197         5          0.0      0.0      0.0          if self.candidate_selection == "all":
   198                                                       candidates = self.cache.get_all_embedding_metadata_objects()
   199         5          0.0      0.0      0.0          elif self.candidate_selection == "top_k":
   200         5        132.4     26.5     23.3              knn = self.cache.get_knn(prompt=prompt, k=max(1, int(self.candidate_k)))
   201         9          0.0      0.0      0.0              for _db_score, embedding_id in knn:
   202         4          0.0      0.0      0.0                  try:
   203         4          0.0      0.0      0.0                      candidates.append(self.cache.get_metadata(embedding_id=embedding_id))
   204                                                           except Exception:
   205                                                               continue
   206                                                   else:
   207                                                       raise ValueError(
   208                                                           f"Unknown candidate_selection={self.candidate_selection!r}. Use 'top_k' or 'all'."
   209                                                       )
   210                                           
   211         5          0.0      0.0      0.0          if not candidates:
   212         1          0.0      0.0      0.0              return None, None
   213                                           
   214         4          0.0      0.0      0.0          best_meta: Optional[EmbeddingMetadataObj] = None
   215         4          0.0      0.0      0.0          best_s: float = -1.0
   216                                           
   217         8          0.0      0.0      0.0          for meta in candidates:
   218         4          0.0      0.0      0.0              cached_prompt = getattr(meta, "prompt", "") or ""
   219         4          0.0      0.0      0.0              if not cached_prompt:
   220                                                           # Can't compute MaxSim without cached prompt text; skip.
   221                                                           continue
   222         4          0.0      0.0      0.0              try:
   223         4        435.4    108.8     76.7                  s = self._maxsim_similarity(prompt, cached_prompt)
   224                                                       except Exception as e:
   225                                                           self.logger.warning(f"MaxSim similarity failed for one candidate: {e}")
   226                                                           continue
   227         4          0.0      0.0      0.0              if s > best_s:
   228         4          0.0      0.0      0.0                  best_s = s
   229         4          0.0      0.0      0.0                  best_meta = meta
   230                                           
   231         4          0.0      0.0      0.0          if best_meta is None:
   232                                                       return None, None
   233         4          0.0      0.0      0.0          return best_meta, best_s

Total time: 0.579842 s
File: /home/ali/vcahce/vcache/vcache_policy/strategies/verified_splitter.py
Function: VerifiedSplitterDecisionPolicy.process_request at line 235

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   235                                               def process_request(
   236                                                   self, prompt: str, system_prompt: Optional[str], id_set: int
   237                                               ) -> Tuple[bool, str, EmbeddingMetadataObj]:
   238         5          0.0      0.0      0.0          if self.inference_engine is None or self.cache is None:
   239                                                       raise ValueError("Policy has not been setup")
   240                                           
   241                                                   # If cache is empty, this will return (None, None)
   242         5        567.9    113.6     97.9          nn_metadata, similarity_score = self._select_nn_by_maxsim(prompt)
   243         5          0.0      0.0      0.0          if nn_metadata is None or similarity_score is None:
   244         1          0.0      0.0      0.0              response = self.inference_engine.create(prompt=prompt, system_prompt=system_prompt)
   245         1         11.0     11.0      1.9              self.cache.add(prompt=prompt, response=response, id_set=id_set)
   246         1          0.0      0.0      0.0              return False, response, EmbeddingMetadataObj(embedding_id=-1, response="")
   247                                           
   248         4          0.2      0.0      0.0          action = self.bayesian.select_action(similarity_score=similarity_score, metadata=nn_metadata)
   249                                           
   250         4          0.0      0.0      0.0          match action:
   251         4          0.0      0.0      0.0              case _Action.EXPLOIT:
   252                                                           return True, nn_metadata.response, nn_metadata
   253         4          0.0      0.0      0.0              case _Action.EXPLORE:
   254         4          0.0      0.0      0.0                  response = self.inference_engine.create(prompt=prompt, system_prompt=system_prompt)
   255         8          0.7      0.1      0.1                  self.__update_cache(
   256         4          0.0      0.0      0.0                      response=response,
   257         4          0.0      0.0      0.0                      nn_metadata=nn_metadata,
   258         4          0.0      0.0      0.0                      similarity_score=similarity_score,
   259         4          0.0      0.0      0.0                      embedding_id=nn_metadata.embedding_id,
   260         4          0.0      0.0      0.0                      prompt=prompt,
   261         4          0.0      0.0      0.0                      label_id_set=id_set,
   262                                                           )
   263         4          0.0      0.0      0.0                  return False, response, nn_metadata

