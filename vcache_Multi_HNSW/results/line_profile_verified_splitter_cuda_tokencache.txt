args={'dataset': 'vCache/SemBenchmarkClassification', 'llm_col': 'response_llama_3_8b', 'splitter_checkpoint': '/data2/ali/checkpoints_words', 'splitter_device': 'cuda', 'delta': 0.02, 'candidate_k': 1, 'max_samples': 30, 'warmup_samples': 3, 'profile_samples': 20, 'sleep': 0.0, 'hf_cache_base': '/data2/ali/hf', 'output': 'results/line_profile_verified_splitter_cuda_tokencache.txt'}
python=3.11.14 (main, Oct 21 2025, 18:31:21) [GCC 11.2.0]
cwd=/home/ali/vcahce

Timer unit: 0.001 s

Total time: 0.895948 s
File: /home/ali/vcahce/benchmarks/line_profile_verified_splitter.py
Function: main.<locals>.<lambda> at line 237

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   237         2        895.9    448.0    100.0          lambda: _run_loop(
   238         1          0.0      0.0      0.0              vcache=vcache,
   239         1          0.0      0.0      0.0              rows=rows_list[int(args.warmup_samples) :],
   240         1          0.0      0.0      0.0              llm_col=args.llm_col,
   241         1          0.0      0.0      0.0              profile_samples=int(args.profile_samples),
   242         1          0.0      0.0      0.0              sleep_s=float(args.sleep),
   243                                                   )

Total time: 0.895497 s
File: /home/ali/vcahce/vcache/main.py
Function: VCache.infer_with_cache_info at line 53

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    53                                               def infer_with_cache_info(
    54                                                   self,
    55                                                   prompt: str,
    56                                                   system_prompt: Optional[str] = None,
    57                                                   id_set: int = -1,
    58                                               ) -> Tuple[bool, str, EmbeddingMetadataObj, EmbeddingMetadataObj]:
    59                                                   """Infers a response and returns the cache hit status and metadata.
    60                                           
    61                                                   Args:
    62                                                       prompt (str): The prompt to create a response for.
    63                                                       system_prompt (Optional[str]): The optional system prompt to use
    64                                                           for the response. Overrides the system prompt in the
    65                                                           VCacheConfig if provided.
    66                                                       id_set (int): The set identifier for the embedding. This is used in the
    67                                                           benchmark to identify if the nearest neighbor is from the same set
    68                                                           (if the cached response is correct or incorrect).
    69                                           
    70                                                   Returns:
    71                                                       Tuple[bool, str, EmbeddingMetadataObj, EmbeddingMetadataObj]: A tuple containing the cache
    72                                                           hit status, the response, the metadata of the response and nearest neighbor metadata.
    73                                                   """
    74        20          0.0      0.0      0.0          if system_prompt is None:
    75                                                       system_prompt = self.vcache_config.system_prompt
    76                                           
    77        20          0.0      0.0      0.0          if self.vcache_config.eviction_policy.is_evicting():
    78                                                       response = self.__generate_response(prompt, system_prompt)
    79                                                       return (
    80                                                           False,
    81                                                           response,
    82                                                           EmbeddingMetadataObj(embedding_id=-1, response=response, id_set=id_set),
    83                                                           EmbeddingMetadataObj(embedding_id=-1, response=response, id_set=id_set),
    84                                                       )
    85                                           
    86        40        889.6     22.2     99.3          is_cache_hit, response, nn_metadata = self.vcache_policy.process_request(
    87        20          0.0      0.0      0.0              prompt, system_prompt, id_set
    88                                                   )
    89                                           
    90        20          0.0      0.0      0.0          if nn_metadata is not None:
    91        20          0.1      0.0      0.0              self.vcache_config.eviction_policy.update_eviction_metadata(nn_metadata)
    92                                           
    93        20          0.0      0.0      0.0          nn_metadata_copy: Optional[EmbeddingMetadataObj] = (
    94        20          5.4      0.3      0.6              copy.deepcopy(nn_metadata) if nn_metadata is not None else None
    95                                                   )
    96                                           
    97        20          0.0      0.0      0.0          if self.vcache_config.eviction_policy.ready_to_evict(self.vcache_policy.cache):
    98                                                       self.vcache_config.eviction_policy.evict(self.vcache_policy.cache)
    99                                           
   100        20          0.0      0.0      0.0          if is_cache_hit:
   101                                                       return is_cache_hit, response, nn_metadata_copy, nn_metadata_copy
   102                                                   else:
   103        20          0.0      0.0      0.0              return (
   104        20          0.0      0.0      0.0                  is_cache_hit,
   105        20          0.0      0.0      0.0                  response,
   106        20          0.3      0.0      0.0                  EmbeddingMetadataObj(embedding_id=-1, response=response, id_set=id_set),
   107        20          0.0      0.0      0.0                  nn_metadata_copy,
   108                                                       )

Total time: 0.277449 s
File: /home/ali/vcahce/vcache/vcache_core/cache/cache.py
Function: Cache.get_knn at line 65

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    65                                               def get_knn(self, prompt: str, k: int) -> List[tuple[float, int]]:
    66                                                   """Gets k-nearest neighbors for a given prompt.
    67                                           
    68                                                   Args:
    69                                                       prompt (str): The prompt to get the k-nearest neighbors for.
    70                                                       k (int): The number of nearest neighbors to retrieve.
    71                                           
    72                                                   Returns:
    73                                                       List[tuple[float, int]]: A list of tuples, each containing a
    74                                                       similarity score and an embedding ID.
    75                                                   """
    76        20        274.8     13.7     99.0          embedding = self.embedding_engine.get_embedding(prompt)
    77        20          2.7      0.1      1.0          return self.embedding_store.get_knn(embedding, k)

Total time: 0.274618 s
File: /home/ali/vcahce/vcache/vcache_core/cache/embedding_engine/strategies/bge.py
Function: BGEEmbeddingEngine.get_embedding at line 16

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    16                                               @override
    17                                               def get_embedding(self, text: str) -> List[float]:
    18                                                   """
    19                                                   Get the embedding for the given text using the underlying BGE model.
    20                                           
    21                                                   Args:
    22                                                       text: The text to get the embedding for.
    23                                           
    24                                                   Returns:
    25                                                       The embedding of the text as a list of floats.
    26                                                   """
    27                                                   # EmbeddingModel.get_embedding returns a numpy array, we convert to list
    28        20        274.1     13.7     99.8          embedding = self.embedding_model.get_embedding(text)
    29        20          0.5      0.0      0.2          return embedding.tolist()

Total time: 0.0024461 s
File: /home/ali/vcahce/vcache/vcache_core/cache/embedding_store/vector_db/strategies/hnsw_lib.py
Function: HNSWLibVectorDB.get_knn at line 80

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    80                                               def get_knn(self, embedding: List[float], k: int) -> List[tuple[float, int]]:
    81                                                   """Gets k-nearest neighbors for a given embedding.
    82                                           
    83                                                   Args:
    84                                                       embedding (List[float]): The query embedding vector.
    85                                                       k (int): The number of nearest neighbors to return.
    86                                           
    87                                                   Returns:
    88                                                       List[tuple[float, int]]: A list of tuples containing similarity
    89                                                       scores and embedding IDs.
    90                                                   """
    91        20          0.0      0.0      1.7          if self.index is None:
    92                                                       return []
    93        20          0.0      0.0      1.9          k_ = min(k, self.embedding_count)
    94        20          0.0      0.0      0.6          if k_ == 0:
    95                                                       return []
    96        20          1.3      0.1     53.5          ids, similarities = self.index.knn_query(embedding, k=k_)
    97        20          0.1      0.0      5.2          metric_type = self.similarity_metric_type.value
    98        40          0.7      0.0     28.9          similarity_scores = [
    99        20          0.0      0.0      2.0              self.transform_similarity_score(sim, metric_type) for sim in similarities[0]
   100                                                   ]
   101        20          0.1      0.0      3.9          id_list = [int(id) for id in ids[0]]
   102        20          0.1      0.0      2.4          return list(zip(similarity_scores, id_list))

Total time: 0.0592196 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/AdaptedPointerNetworkPolicy.py
Function: CrossAttentionBlock.forward at line 27

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    27                                               def forward(
    28                                                   self,
    29                                                   emb_a: torch.Tensor,
    30                                                   emb_b: torch.Tensor,
    31                                                   pad_mask_a: torch.Tensor | None = None,
    32                                                   pad_mask_b: torch.Tensor | None = None,
    33                                               ) -> tuple[torch.Tensor, torch.Tensor]:
    34                                                   # A attends to B
    35        40         22.2      0.6     37.5          attn_a, _ = self.mha_ab(query=emb_a, key=emb_b, value=emb_b, key_padding_mask=pad_mask_b)
    36        40          2.6      0.1      4.4          a_out = self.norm_ab(emb_a + attn_a)
    37        40          6.9      0.2     11.7          a_out = self.norm_ab(a_out + self.ff_ab(a_out))
    38                                           
    39                                                   # B attends to A
    40        40         18.7      0.5     31.6          attn_b, _ = self.mha_ba(query=emb_b, key=emb_a, value=emb_a, key_padding_mask=pad_mask_a)
    41        40          2.3      0.1      3.9          b_out = self.norm_ba(emb_b + attn_b)
    42        40          6.4      0.2     10.8          b_out = self.norm_ba(b_out + self.ff_ba(b_out))
    43                                           
    44        40          0.0      0.0      0.0          return a_out, b_out

Total time: 0.223996 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/AdaptedPointerNetworkPolicy.py
Function: AdaptedPointerNetworkPolicy.forward at line 179

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   179                                               def forward(self, td, env=None, phase: str = "train", select_best: bool = False, **kwargs):
   180                                                   """与 RL4CO 的 REINFORCE 调用约定兼容的入口。
   181                                                   根据阶段/标志选择采样或贪心解码，返回包含 reward 与 log_likelihood 的字典。
   182                                                   """
   183                                                   # In inference paths (e.g., MaxSimSplitter), we only need `actions`.
   184                                                   # Reward computation can be expensive and is not used by the splitter.
   185        20          0.0      0.0      0.0          compute_reward: bool = bool(kwargs.pop("compute_reward", True))
   186                                                  
   187        20          0.0      0.0      0.0          decode_override = kwargs.pop("decode_type", None)
   188        20          0.0      0.0      0.0          if decode_override is not None:
   189        20          0.0      0.0      0.0              decode_type = decode_override
   190                                                   else:
   191                                                       decode_type = "greedy" if (select_best or phase != "train") else "sampling"
   192                                           
   193        20        223.5     11.2     99.8          actions, log_likelihood, info = self._forward(td, decode_type=decode_type, **kwargs)
   194        20          0.0      0.0      0.0          reward = None
   195        20          0.0      0.0      0.0          if compute_reward:
   196                                                       # 计算奖励
   197                                                       used_env = env if env is not None else getattr(self, "env", None)
   198                                                       if used_env is None:
   199                                                           raise RuntimeError("Environment instance is required to compute reward.")
   200                                                       reward = used_env.get_reward(td, actions)
   201                                                   else:
   202                                                       # Cheap placeholder (keeps output schema stable)
   203        20          0.0      0.0      0.0              try:
   204        20          0.4      0.0      0.2                  reward = actions.new_zeros((actions.shape[0],), dtype=torch.float32)
   205                                                       except Exception:
   206                                                           reward = torch.zeros(1, dtype=torch.float32, device=self.device)
   207                                           
   208        20          0.0      0.0      0.0          return {
   209        20          0.0      0.0      0.0              "actions": actions,
   210        20          0.0      0.0      0.0              "log_likelihood": log_likelihood,
   211        20          0.0      0.0      0.0              "reward": reward,
   212        20          0.0      0.0      0.0              "info": info,
   213                                                   }

Total time: 0.220407 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/AdaptedPointerNetworkPolicy.py
Function: AdaptedPointerNetworkPolicy._forward at line 215

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   215                                               def _forward(self, td, decode_type="sampling", **kwargs):
   216                                                   """
   217                                                   实现自回归解码的核心逻辑 。
   218                                                   """
   219                                                   # Ignore legacy debug flag if provided (no-op)
   220        20          0.0      0.0      0.0          kwargs.pop("debug", None)
   221                                                  
   222                                                
   223        20          1.0      0.1      0.5          target_device = next(self.lm.parameters()).device
   224        20          0.0      0.0      0.0          if getattr(self, "_current_module_device", None) != target_device:
   225                                                       self.to(target_device)
   226                                                       self._current_module_device = target_device
   227        20          0.4      0.0      0.2          self._device = target_device
   228                                                   
   229                                              
   230        20          0.0      0.0      0.0          try:
   231        20          5.7      0.3      2.6              td = td.to(target_device)
   232                                                   except Exception:
   233                                                       for k in ['token_embeddings_a','token_embeddings_b','attention_mask_a','attention_mask_b',
   234                                                                 'input_ids_a','input_ids_b','length_a','length_b']:
   235                                                           if k in td and isinstance(td[k], torch.Tensor):
   236                                                               td[k] = td[k].to(target_device)
   237        20          0.5      0.0      0.2          self._init_punctuation_ids(td['input_ids_a'].device)
   238                                                   
   239                                                
   240        20          0.3      0.0      0.1          input_dim = td['token_embeddings_a'].size(-1)
   241        20          0.1      0.0      0.0          if self.input_proj is None:
   242                                                       with torch.inference_mode(False):
   243                                                           if input_dim == self.hidden_dim:
   244                                                               self.input_proj = nn.Identity()
   245                                                           else:
   246                                                               self.input_proj = nn.Linear(input_dim, self.hidden_dim)
   247                                                       self.input_proj.to(self.device)
   248                                           
   249        20          0.5      0.0      0.2          embedded_a = self.input_proj(td['token_embeddings_a'])
   250        20          0.4      0.0      0.2          embedded_b = self.input_proj(td['token_embeddings_b'])
   251                                                   
   252        20          1.2      0.1      0.5          mask_a = ~td['attention_mask_a'].bool()
   253        20          1.0      0.0      0.4          mask_b = ~td['attention_mask_b'].bool()
   254                                                   
   255        20          0.0      0.0      0.0          batch_size = embedded_a.size(0)
   256        20          0.0      0.0      0.0          seq_len_a = embedded_a.size(1)
   257        20          0.0      0.0      0.0          seq_len_b = embedded_b.size(1)
   258                                                   
   259                                                   # --- 2. Encoder (Cross-Attention) ---
   260        60          0.2      0.0      0.1          for layer in self.encoder_layers:
   261        80         60.1      0.8     27.3              embedded_a, embedded_b = layer(
   262        40          0.0      0.0      0.0                  emb_a=embedded_a, 
   263        40          0.0      0.0      0.0                  emb_b=embedded_b, 
   264        40          0.0      0.0      0.0                  pad_mask_a=mask_a, 
   265        40          0.0      0.0      0.0                  pad_mask_b=mask_b
   266                                                       )
   267        20          0.0      0.0      0.0          encoder_outputs_a = embedded_a
   268        20          0.0      0.0      0.0          encoder_outputs_b = embedded_b
   269                                                   
   270                                                   # --- 3. Decoder 初始化 ---
   271        20          0.9      0.0      0.4          decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)
   272        40          0.6      0.0      0.3          h, c = (torch.zeros(batch_size, self.decoder_cell.hidden_size, device=self.device),
   273        20          0.3      0.0      0.1                  torch.zeros(batch_size, self.decoder_cell.hidden_size, device=self.device))
   274                                                   
   275        20          0.0      0.0      0.0          pointers = []
   276        20          0.0      0.0      0.0          log_probs = []
   277                                                   
   278                                                   # 初始边界设为 0 (对应 [CLS])
   279        20          0.3      0.0      0.1          current_bA = torch.zeros(batch_size, dtype=torch.long, device=self.device)
   280        20          0.2      0.0      0.1          current_bB = torch.zeros(batch_size, dtype=torch.long, device=self.device)
   281                                           
   282                                                   # Precompute base punctuation mask (special end tokens are handled per-step)
   283        20          1.7      0.1      0.8          is_punct_base_a = torch.isin(td["input_ids_a"], self._valid_split_ids)
   284        20          1.0      0.0      0.4          is_punct_base_b = torch.isin(td["input_ids_b"], self._valid_split_ids)
   285                                           
   286                                                   # --- 4. 解码循环 ---
   287       100          0.1      0.0      0.0          for step in range(self.max_segments):
   288                                                       # (a) LSTM Step
   289        80          8.5      0.1      3.8              h, c = self.decoder_cell(decoder_input, (h, c))
   290        80          3.8      0.0      1.7              query_vec = self.attention_linear_decoder(h)
   291                                           
   292                                                       # Allow [SEP]/[EOS] only on the final step to prevent early degeneration.
   293        80          0.1      0.0      0.0              if step == self.max_segments - 1 and getattr(self, "_end_split_ids", None) is not None:
   294        20          1.6      0.1      0.7                  is_end_a = torch.isin(td["input_ids_a"], self._end_split_ids)
   295        20          1.0      0.0      0.4                  is_end_b = torch.isin(td["input_ids_b"], self._end_split_ids)
   296        20          0.3      0.0      0.1                  is_punct_global_a = is_punct_base_a | is_end_a
   297        20          0.1      0.0      0.1                  is_punct_global_b = is_punct_base_b | is_end_b
   298                                                       else:
   299        60          0.0      0.0      0.0                  is_punct_global_a = is_punct_base_a
   300        60          0.0      0.0      0.0                  is_punct_global_b = is_punct_base_b
   301                                           
   302                                                       #  指针 A 选择
   303        80          2.1      0.0      1.0              range_a = torch.arange(seq_len_a, device=self.device).expand(batch_size, -1)
   304                                                       
   305                                                       # 1. 必须是标点符号 (预计算结果)
   306                                                       # 2. 必须严格在当前边界之后 (range_a > current_bA)
   307                                                       # 3. 必须在有效长度内 (range_a < length_a)
   308                                                  
   309                                                       
   310        80          1.4      0.0      0.7              is_future_a = range_a > current_bA.unsqueeze(1)
   311        80          2.4      0.0      1.1              is_in_length_a = range_a < td['length_a'].unsqueeze(1)
   312        80          1.5      0.0      0.7              valid_slots_a = is_punct_global_a & is_future_a & is_in_length_a
   313        80          0.8      0.0      0.4              mask_ptr_a = ~valid_slots_a
   314                                           
   315        80          0.2      0.0      0.1              inp_a = query_vec.unsqueeze(2)
   316        80         38.4      0.5     17.4              ctx_a = self.attention_linear_encoder(encoder_outputs_a.permute(0, 2, 1))
   317        80          1.3      0.0      0.6              V_exp = self.V.unsqueeze(0).expand(batch_size, -1).unsqueeze(1)
   318        80          4.1      0.1      1.8              attn_scores_a = torch.bmm(V_exp, torch.tanh(inp_a + ctx_a)).squeeze(1)
   319                                                       
   320                                                  
   321        80          0.1      0.0      0.1              mask_fill_val = float('-inf') 
   322        80          2.3      0.0      1.0              attn_scores_a = attn_scores_a.masked_fill(mask_ptr_a, mask_fill_val)
   323                                           
   324                                                       # === 指针 B 选择 ===
   325        80          1.8      0.0      0.8              range_b = torch.arange(seq_len_b, device=self.device).expand(batch_size, -1)
   326                                                       
   327        80          1.2      0.0      0.5              is_future_b = range_b > current_bB.unsqueeze(1)
   328        80          2.5      0.0      1.1              is_in_length_b = range_b < td['length_b'].unsqueeze(1)
   329                                                       
   330        80          1.4      0.0      0.6              valid_slots_b = is_punct_global_b & is_future_b & is_in_length_b
   331        80          0.8      0.0      0.4              mask_ptr_b = ~valid_slots_b
   332                                           
   333        80          0.2      0.0      0.1              inp_b = query_vec.unsqueeze(2)
   334        80          9.4      0.1      4.3              ctx_b = self.attention_linear_encoder(encoder_outputs_b.permute(0, 2, 1))
   335        80          3.5      0.0      1.6              attn_scores_b = torch.bmm(V_exp, torch.tanh(inp_b + ctx_b)).squeeze(1)
   336        80          2.0      0.0      0.9              attn_scores_b = attn_scores_b.masked_fill(mask_ptr_b, mask_fill_val)
   337                                           
   338                                                       # === 采样/Argmax ===
   339        80          1.5      0.0      0.7              pointer_a = torch.zeros(batch_size, dtype=torch.long, device=self.device)
   340        80          0.8      0.0      0.4              pointer_b = torch.zeros(batch_size, dtype=torch.long, device=self.device)
   341        80          0.9      0.0      0.4              logp_a = torch.zeros(batch_size, device=self.device)
   342        80          0.8      0.0      0.4              logp_b = torch.zeros(batch_size, device=self.device)
   343                                           
   344        80          1.3      0.0      0.6              has_valid_a = valid_slots_a.any(dim=1)
   345        80          0.8      0.0      0.3              has_valid_b = valid_slots_b.any(dim=1)
   346                                           
   347                                                       # 处理 A
   348        80          2.6      0.0      1.2              if has_valid_a.any():
   349        56          0.1      0.0      0.0                  rows = has_valid_a
   350        56          0.0      0.0      0.0                  if decode_type == "sampling":
   351                                                               dist = Categorical(logits=attn_scores_a[rows])
   352                                                               act = dist.sample()
   353                                                               pointer_a[rows] = act
   354                                                               logp_a[rows] = dist.log_prob(act)
   355                                                           else:
   356        56          7.2      0.1      3.3                      pointer_a[rows] = torch.argmax(attn_scores_a[rows], dim=1)
   357                                           
   358        80          2.8      0.0      1.3              if (~has_valid_a).any():
   359        24          0.2      0.0      0.1                  rows = ~has_valid_a
   360                                                         
   361        24          3.4      0.1      1.5                  pointer_a[rows] = td['length_a'][rows] - 1
   362                                                        
   363                                           
   364                                                       # 处理 B
   365        80          1.8      0.0      0.8              if has_valid_b.any():
   366        50          0.0      0.0      0.0                  rows = has_valid_b
   367        50          0.0      0.0      0.0                  if decode_type == "sampling":
   368                                                               dist = Categorical(logits=attn_scores_b[rows])
   369                                                               act = dist.sample()
   370                                                               pointer_b[rows] = act
   371                                                               logp_b[rows] = dist.log_prob(act)
   372                                                           else:
   373        50          5.1      0.1      2.3                      pointer_b[rows] = torch.argmax(attn_scores_b[rows], dim=1)
   374                                                       
   375        80          2.5      0.0      1.1              if (~has_valid_b).any():
   376        30          0.2      0.0      0.1                  rows = ~has_valid_b
   377        30          3.5      0.1      1.6                  pointer_b[rows] = td['length_b'][rows] - 1
   378                                           
   379        80          1.7      0.0      0.8              pointers.append(torch.stack([pointer_a, pointer_b], dim=1))
   380        80          0.8      0.0      0.4              log_probs.append(logp_a + logp_b)
   381                                           
   382                                                       # ---------------------------------------------------------------------
   383                                                       # Fast feedback embedding: reuse token-level embeddings already computed
   384                                                       # outside the RL loop (MaxSimSplitter.split_pair_return_segments does one
   385                                                       # LM forward per text). We slice the span corresponding to the chosen
   386                                                       # boundary and mean-pool it, matching MaxSimEnv's "FAST VERSION":
   387                                                       #   real_start = (prev + 1) if prev > 0 else 1   # skip [CLS] at index 0
   388                                                       #   end        = pointer + 1                    # inclusive boundary token
   389                                                       # ---------------------------------------------------------------------
   390        80          1.4      0.0      0.6              token_emb_a = td["token_embeddings_a"]  # [B, L, H] on self.device already
   391        80          1.0      0.0      0.4              length_a = td["length_a"].long()  # [B]
   392                                           
   393        80          0.0      0.0      0.0              feedback_emb_a_list = []
   394       160          0.2      0.0      0.1              for b in range(batch_size):
   395        80          1.5      0.0      0.7                  s_a = int(current_bA[b].item())
   396        80          1.0      0.0      0.5                  e_a = int(pointer_a[b].item())
   397        80          1.2      0.0      0.5                  la = int(length_a[b].item())
   398        80          0.1      0.0      0.0                  la = max(1, la)
   399                                           
   400                                                           # Policy semantics: skip [CLS] (idx 0), boundary token is inclusive.
   401        80          0.0      0.0      0.0                  real_start_a = (s_a + 1) if s_a > 0 else 1
   402        80          0.0      0.0      0.0                  real_end_a = e_a + 1
   403                                           
   404                                                           # Clip to effective length to avoid pooling padded tail.
   405        80          0.1      0.0      0.0                  real_start_a = min(max(0, real_start_a), la)
   406        80          0.1      0.0      0.0                  real_end_a = min(max(0, real_end_a), la)
   407                                           
   408        80          0.0      0.0      0.0                  if real_end_a <= real_start_a:
   409                                                               # Match previous behavior: empty segment -> zeros embedding.
   410        28          0.2      0.0      0.1                      emb_val = torch.zeros(
   411        14          0.0      0.0      0.0                          token_emb_a.size(-1), device=token_emb_a.device, dtype=token_emb_a.dtype
   412                                                               )
   413                                                           else:
   414        66          1.7      0.0      0.8                      emb_val = token_emb_a[b, real_start_a:real_end_a, :].mean(dim=0)
   415                                           
   416        80          0.1      0.0      0.0                  feedback_emb_a_list.append(emb_val)
   417                                           
   418        80          1.8      0.0      0.8              feedback_tensor_a = torch.stack(feedback_emb_a_list, dim=0)  # [B, H]
   419                                                       
   420                                                       # 投影作为 LSTM 下一步输入
   421        80          1.5      0.0      0.7              decoder_input = self.input_proj(feedback_tensor_a)
   422                                           
   423                                                       # 更新状态
   424        80          0.1      0.0      0.0              current_bA = pointer_a
   425        80          0.1      0.0      0.0              current_bB = pointer_b
   426                                           
   427                                                
   428        20          0.4      0.0      0.2          actions = torch.stack(pointers, 1).view(batch_size, -1)
   429        20          0.6      0.0      0.3          log_likelihood = torch.stack(log_probs, 1).sum(dim=1)
   430                                           
   431        20          0.0      0.0      0.0          info = {}
   432                                           
   433        20          0.0      0.0      0.0          return actions, log_likelihood, info

Total time: 0 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/MaxSimEnv.py
Function: MaxSimEnv.raw_score_text at line 258

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   258                                               def raw_score_text(self,
   259                                                                query_tensor: torch.Tensor,
   260                                                                sub_corpus_embeddings: torch.Tensor,
   261                                                                query_weights: torch.Tensor,
   262                                                                corpus_weights: torch.Tensor,
   263                                                                times: int = 0) -> torch.Tensor:
   264                                                   """
   265                                                   计算query和corpus之间基于粗粒度和细粒度嵌入的加权相似度分数。
   266                                                   """
   267                                                  
   268                                                   weights = F.softmax(self.score_weights_raw, dim=0)
   269                                                   w_coarse = weights[0]
   270                                                   w_fine_row = weights[1]
   271                                                   w_fine_col = weights[2]
   272                                           
   273                                                   query_full_vec = query_tensor[-1:, :]
   274                                                   corpus_full_vec = sub_corpus_embeddings[-1:, :]
   275                                                   coarse_grained_score = F.cosine_similarity(query_full_vec, corpus_full_vec).squeeze()
   276                                           
   277                                                   query_sentence_vecs = query_tensor[:-1, :]
   278                                                   corpus_sentence_vecs = sub_corpus_embeddings[:-1, :]
   279                                           
   280                                                   if query_sentence_vecs.shape[0] > 0 and corpus_sentence_vecs.shape[0] > 0:
   281                                                       query_norm = F.normalize(query_sentence_vecs, p=2, dim=-1)
   282                                                       corpus_norm = F.normalize(corpus_sentence_vecs, p=2, dim=-1)
   283                                                       cos_sim_matrix = torch.mm(query_norm, corpus_norm.T)
   284                                           
   285                                                       max_cos_sim_row = torch.max(cos_sim_matrix, dim=1).values
   286                                                       fine_grained_row_score = torch.sum(max_cos_sim_row * query_weights) / (torch.sum(query_weights) + 1e-8)
   287                                           
   288                                                       max_cos_sim_col = torch.max(cos_sim_matrix, dim=0).values
   289                                                       fine_grained_col_score = torch.sum(max_cos_sim_col * corpus_weights) / (torch.sum(corpus_weights) + 1e-8)
   290                                                   else:
   291                                                       fine_grained_row_score = torch.tensor(0.0, device=self.device)
   292                                                       fine_grained_col_score = torch.tensor(0.0, device=self.device)
   293                                           
   294                                                 
   295                                                   final_score = (w_coarse * coarse_grained_score +
   296                                                                  w_fine_row * fine_grained_row_score +
   297                                                                  w_fine_col * fine_grained_col_score)
   298                                           
   299                                                   return final_score

Total time: 0 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/MaxSimEnv.py
Function: MaxSimEnv._get_reward at line 301

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   301                                               def _get_reward(self, td: TensorDict, actions) -> torch.Tensor:
   302                                                   batch_size = actions.shape[0]
   303                                                   lm = self.reward_lm.model
   304                                                   tok = self.reward_lm.tokenizer
   305                                                   lm_device = next(lm.parameters()).device
   306                                                   rewards = torch.zeros(batch_size, device=lm_device)
   307                                           
   308                                                   input_ids_a = td["input_ids_a"]
   309                                                   input_ids_b = td["input_ids_b"]
   310                                                   length_a = td["length_a"].long()
   311                                                   length_b = td["length_b"].long()
   312                                           
   313                                                   # Normalize actions -> pa_tensor/pb_tensor of shape [batch, max_segments]
   314                                                   if isinstance(actions, torch.Tensor) and actions.dim() == 3 and actions.size(-1) == 2:
   315                                                       # shape [batch, K, 2] where last dim is [A, B]
   316                                                       pa_tensor = actions[:, :, 0]
   317                                                       pb_tensor = actions[:, :, 1]
   318                                                   elif isinstance(actions, torch.Tensor) and actions.dim() == 2:
   319                                                       # shape [batch, 2*K] expected; policy currently outputs interleaved [A0,B0,A1,B1,...]
   320                                                       if actions.size(1) != 2 * self.max_segments:
   321                                                           raise ValueError(
   322                                                               f"Unsupported actions shape for reward: {actions.shape}. "
   323                                                               f"Expected second dim = {2*self.max_segments} (2*max_segments)."
   324                                                           )
   325                                                       pa_tensor = actions[:, 0::2]  # [A0, A1, ...]
   326                                                       pb_tensor = actions[:, 1::2]  # [B0, B1, ...]
   327                                                   else:
   328                                                       raise ValueError(
   329                                                           f"Unsupported actions shape for reward: {getattr(actions, 'shape', None)}. "
   330                                                           f"Expected [batch, {2*self.max_segments}] or [batch, {self.max_segments}, 2]."
   331                                                       )
   332                                           
   333                                                   for i in range(batch_size):
   334                                                       pa = pa_tensor[i].tolist()
   335                                                       pb = pb_tensor[i].tolist()
   336                                           
   337                                                       la = int(length_a[i].item())
   338                                                       lb = int(length_b[i].item())
   339                                                       la = max(1, la)
   340                                                       lb = max(1, lb)
   341                                           
   342                                                       pa = [min(max(0, p), la - 1) for p in pa]
   343                                                       pb = [min(max(0, p), lb - 1) for p in pb]
   344                                           
   345                                                       # NOTE: Align segmentation semantics with the policy.
   346                                                       # Policy treats pointers as "end boundary tokens" and skips [CLS] (index 0) when building segments:
   347                                                       #   real_start = prev_boundary + 1 (but starts from 1 when prev_boundary==0)
   348                                                       #   real_end   = pointer + 1      (inclusive of the boundary token)
   349                                                       bounds_a = sorted(set(pa))
   350                                                       bounds_b = sorted(set(pb))
   351                                           
   352                                                       # ===== ORIGINAL HEAVY LM FORWARD (保留为注释，作为兜底参考) =====
   353                                                       # seg_ids_a = []
   354                                                       # start = 0
   355                                                       # for p in bounds_a:
   356                                                       #     end = p + 1
   357                                                       #     if end > start:
   358                                                       #         seg_ids_a.append(input_ids_a[i, start:end])
   359                                                       #     start = end
   360                                                       # if start < la:
   361                                                       #     seg_ids_a.append(input_ids_a[i, start:la])
   362                                                       #
   363                                                       # seg_ids_b = []
   364                                                       # start = 0
   365                                                       # for p in bounds_b:
   366                                                       #     end = p + 1
   367                                                       #     if end > start:
   368                                                       #         seg_ids_b.append(input_ids_b[i, start:end])
   369                                                       #     start = end
   370                                                       # if start < lb:
   371                                                       #     seg_ids_b.append(input_ids_b[i, start:lb])
   372                                                       #
   373                                                       # if len(seg_ids_a) == 0 or len(seg_ids_b) == 0:
   374                                                       #     rewards[i] = 0.0
   375                                                       #     continue
   376                                                       #
   377                                                       # with torch.no_grad():
   378                                                       #     seg_emb_a_list = []
   379                                                       #     for s in seg_ids_a:
   380                                                       #         ids = s.unsqueeze(0).to(lm_device)
   381                                                       #         attn = torch.ones_like(ids, device=lm_device)
   382                                                       #         out = lm(ids, attention_mask=attn).last_hidden_state.mean(dim=1)
   383                                                       #         seg_emb_a_list.append(out.squeeze(0))
   384                                                       #     sentence_embeds_a = torch.stack(seg_emb_a_list, dim=0)
   385                                                       #
   386                                                       #     seg_emb_b_list = []
   387                                                       #     for s in seg_ids_b:
   388                                                       #         ids = s.unsqueeze(0).to(lm_device)
   389                                                       #         attn = torch.ones_like(ids, device=lm_device)
   390                                                       #         out = lm(ids, attention_mask=attn).last_hidden_state.mean(dim=1)
   391                                                       #         seg_emb_b_list.append(out.squeeze(0))
   392                                                       #     sentence_embeds_b = torch.stack(seg_emb_b_list, dim=0)
   393                                                       #
   394                                                       #     full_ids_a = input_ids_a[i, :la].unsqueeze(0).to(lm_device)
   395                                                       #     full_attn_a = (full_ids_a != tok.pad_token_id).long()
   396                                                       #     full_embed_a = lm(full_ids_a, attention_mask=full_attn_a).last_hidden_state.mean(dim=1)
   397                                                       #
   398                                                       #     full_ids_b = input_ids_b[i, :lb].unsqueeze(0).to(lm_device)
   399                                                       #     full_attn_b = (full_ids_b != tok.pad_token_id).long()
   400                                                       #     full_embed_b = lm(full_ids_b, attention_mask=full_attn_b).last_hidden_state.mean(dim=1)
   401                                                       #
   402                                                       #     if sentence_embeds_a.shape[0] > 0:
   403                                                       #         query_tensor = torch.cat([sentence_embeds_a, full_embed_a], dim=0)
   404                                                       #     else:
   405                                                       #         query_tensor = full_embed_a
   406                                                       #
   407                                                       #     if sentence_embeds_b.shape[0] > 0:
   408                                                       #         corpus_tensor = torch.cat([sentence_embeds_b, full_embed_b], dim=0)
   409                                                       #     else:
   410                                                       #         corpus_tensor = full_embed_b
   411                                                       # ============================================================
   412                                           
   413                                                       # FAST VERSION：直接复用 token 级嵌入做均值池化
   414                                                       with torch.no_grad():
   415                                                           token_emb_a = td["token_embeddings_a"][i, :la].to(lm_device)
   416                                                           token_emb_b = td["token_embeddings_b"][i, :lb].to(lm_device)
   417                                           
   418                                                           seg_emb_a_list = []
   419                                                           prev = 0
   420                                                           for p in bounds_a:
   421                                                               end = p + 1
   422                                                               real_start = (prev + 1) if prev > 0 else 1  # skip [CLS]
   423                                                               if end > real_start:
   424                                                                   seg_emb_a_list.append(token_emb_a[real_start:end].mean(dim=0))
   425                                                               prev = p
   426                                                           # Tail segment after last boundary
   427                                                           tail_start = (prev + 1) if prev > 0 else 1
   428                                                           if tail_start < la:
   429                                                               seg_emb_a_list.append(token_emb_a[tail_start:la].mean(dim=0))
   430                                                           if len(seg_emb_a_list) == 0:
   431                                                               rewards[i] = 0.0
   432                                                               continue
   433                                                           sentence_embeds_a = torch.stack(seg_emb_a_list, dim=0)
   434                                           
   435                                                           seg_emb_b_list = []
   436                                                           prev = 0
   437                                                           for p in bounds_b:
   438                                                               end = p + 1
   439                                                               real_start = (prev + 1) if prev > 0 else 1  # skip [CLS]
   440                                                               if end > real_start:
   441                                                                   seg_emb_b_list.append(token_emb_b[real_start:end].mean(dim=0))
   442                                                               prev = p
   443                                                           tail_start = (prev + 1) if prev > 0 else 1
   444                                                           if tail_start < lb:
   445                                                               seg_emb_b_list.append(token_emb_b[tail_start:lb].mean(dim=0))
   446                                                           if len(seg_emb_b_list) == 0:
   447                                                               rewards[i] = 0.0
   448                                                               continue
   449                                                           sentence_embeds_b = torch.stack(seg_emb_b_list, dim=0)
   450                                           
   451                                                           full_embed_a = token_emb_a.mean(dim=0, keepdim=True)
   452                                                           full_embed_b = token_emb_b.mean(dim=0, keepdim=True)
   453                                           
   454                                                           if sentence_embeds_a.shape[0] > 0:
   455                                                               query_tensor = torch.cat([sentence_embeds_a, full_embed_a], dim=0)
   456                                                           else:
   457                                                               query_tensor = full_embed_a
   458                                           
   459                                                           if sentence_embeds_b.shape[0] > 0:
   460                                                               corpus_tensor = torch.cat([sentence_embeds_b, full_embed_b], dim=0)
   461                                                           else:
   462                                                               corpus_tensor = full_embed_b
   463                                           
   464                                                           query_weights = torch.ones(sentence_embeds_a.shape[0], device=query_tensor.device)
   465                                                           corpus_weights = torch.ones(sentence_embeds_b.shape[0], device=corpus_tensor.device)
   466                                           
   467                                                           rewards[i] = self.raw_score_text(
   468                                                               query_tensor, corpus_tensor, query_weights, corpus_weights, times=0
   469                                                           ).to(rewards.device)
   470                                           
   471                                                   return rewards

Total time: 0 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/MaxSimSplitter.py
Function: MaxSimSplitter.split_pair_return_segments at line 183

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   183                                               def split_pair_return_segments(self, text_a, text_b):
   184                                                   """
   185                                                   输入：Query 和 Cache Candidate
   186                                                   输出：RL模型优化后的 A片段列表 和 B片段列表
   187                                                   """
   188                                                   # 1. 构造输入 (Joint Input)
   189                                                   # Batch the two texts to avoid two separate tokenizer + LM forward passes.
   190                                                   inputs = self.generator.tokenizer(
   191                                                       [text_a, text_b],
   192                                                       return_tensors="pt",
   193                                                       padding="max_length",
   194                                                       truncation=True,
   195                                                       max_length=512,
   196                                                   ).to(self.device)
   197                                           
   198                                                   with torch.inference_mode():
   199                                                       hs = self.generator.lm(**inputs).last_hidden_state  # [2, L, H]
   200                                           
   201                                                   # Split batch back into the A/B shapes expected downstream (batch_size=1 each).
   202                                                   embeds_a = hs[0:1, :, :]
   203                                                   embeds_b = hs[1:2, :, :]
   204                                                   input_ids_a = inputs["input_ids"][0:1, :]
   205                                                   input_ids_b = inputs["input_ids"][1:2, :]
   206                                                   attention_mask_a = inputs["attention_mask"][0:1, :]
   207                                                   attention_mask_b = inputs["attention_mask"][1:2, :]
   208                                           
   209                                                   td = TensorDict(
   210                                                       {
   211                                                           "token_embeddings_a": embeds_a,
   212                                                           "token_embeddings_b": embeds_b,
   213                                                           "attention_mask_a": attention_mask_a,
   214                                                           "attention_mask_b": attention_mask_b,
   215                                                           "length_a": attention_mask_a.sum(dim=1),
   216                                                           "length_b": attention_mask_b.sum(dim=1),
   217                                                           "input_ids_a": input_ids_a,
   218                                                           "input_ids_b": input_ids_b,
   219                                                       },
   220                                                       batch_size=1,
   221                                                   )
   222                                           
   223                                                   # 2. Greedy Decoding 获取最佳切分动作
   224                                                   with torch.inference_mode():
   225                                                       out = self.policy(
   226                                                           td,
   227                                                           None,
   228                                                           phase="test",
   229                                                           select_best=True,
   230                                                           decode_type="greedy",
   231                                                           compute_reward=False,
   232                                                       )
   233                                                   
   234                                                   actions = out['actions'][0] # [2 * max_segments]
   235                                           
   236                                                   # 3. 解析动作 -> 文本片段
   237                                                   # NOTE: In this repo, actions are interleaved: [A0, B0, A1, B1, ...]
   238                                                   # See `inspect_punctuation_cases.py` and `MaxSimEnv._step` (full-plan interleaved layout).
   239                                                   if not isinstance(actions, torch.Tensor):
   240                                                       actions = torch.as_tensor(actions, device=self.device)
   241                                                   total = int(actions.numel())
   242                                                   if total % 2 != 0:
   243                                                       raise ValueError(f"Expected even number of action entries (A/B interleaved), got {total}")
   244                                                   max_segments = total // 2
   245                                                   pointers_a = actions[0: 2 * max_segments: 2].tolist()
   246                                                   pointers_b = actions[1: 2 * max_segments: 2].tolist()
   247                                                   
   248                                                   # Reconstruct segments in **token-index space** (pointers are token positions).
   249                                                   # This avoids the previous mismatch where pointers (token indices) were applied
   250                                                   # to `prompt.lower().split()` (word indices).
   251                                                   segments_a = get_segments_from_token_pointers(
   252                                                       tokenizer=self.generator.tokenizer,
   253                                                       input_ids=input_ids_a[0],
   254                                                       attention_mask=attention_mask_a[0],
   255                                                       pointers=pointers_a,
   256                                                   )
   257                                                   segments_b = get_segments_from_token_pointers(
   258                                                       tokenizer=self.generator.tokenizer,
   259                                                       input_ids=input_ids_b[0],
   260                                                       attention_mask=attention_mask_b[0],
   261                                                       pointers=pointers_b,
   262                                                   )
   263                                                   
   264                                                   return segments_a, segments_b

Total time: 0.273331 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/embedding_model.py
Function: EmbeddingModel.get_embedding at line 86

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    86                                               def get_embedding(self, text):
    87                                                   """ 获取文本的向量嵌入 """
    88                                                 
    89        20          0.9      0.0      0.3          device = next(self.model.parameters()).device
    90                                               
    91        20         21.1      1.1      7.7          inputs = self.tokenizer(text, return_tensors="pt", padding=True, truncation=True)
    92                                                   
    93        20          3.3      0.2      1.2          inputs = {k: v.to(device) for k, v in inputs.items()}
    94                                           
    95        40          0.6      0.0      0.2          with torch.no_grad():
    96        20        242.2     12.1     88.6              outputs = self.model(**inputs)
    97                                           
    98        20          0.0      0.0      0.0          hs = outputs.last_hidden_state  # [1, L, H]
    99        20          0.0      0.0      0.0          attn = inputs.get("attention_mask", None)
   100        20          0.0      0.0      0.0          if attn is None:
   101                                                       pooled = hs.mean(dim=1)
   102                                                   else:
   103        20          1.0      0.1      0.4              attn_f = attn.to(dtype=hs.dtype).unsqueeze(-1)
   104        20          2.7      0.1      1.0              pooled = (hs * attn_f).sum(dim=1) / attn_f.sum(dim=1).clamp_min(1.0)
   105                                           
   106        20          1.5      0.1      0.6          return pooled.squeeze().cpu().numpy()

Total time: 0 s
File: /home/ali/vcahce/vcache/vcache_core/splitter/embedding_model.py
Function: EmbeddingModel.get_embeddings_tensor at line 146

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   146                                               def get_embeddings_tensor(
   147                                                   self, texts: list[str], device: torch.device | str | None = None
   148                                               ) -> torch.Tensor:
   149                                                   """
   150                                                   Batched version of `get_embedding_tensor`.
   151                                           
   152                                                   Semantics: matches what you'd get by calling `get_embedding_tensor` on each
   153                                                   text independently (batch_size=1 => no padding) by using **masked mean pooling**
   154                                                   over non-padding tokens when batching introduces padding.
   155                                           
   156                                                   Returns:
   157                                                       torch.Tensor of shape [len(texts), hidden_size] on `device`.
   158                                                   """
   159                                                   if not texts:
   160                                                       raise ValueError("texts must be a non-empty list[str]")
   161                                           
   162                                                   if device is None:
   163                                                       device = next(self.model.parameters()).device
   164                                                   else:
   165                                                       device = torch.device(device)
   166                                           
   167                                                   # Ensure model is on the target device.
   168                                                   #
   169                                                   # Important: `torch.device("cuda")` has `index=None`, while a model is typically
   170                                                   # on `cuda:0`. Treat these as equivalent to avoid an expensive `.to(...)` call
   171                                                   # on every invocation in hot paths.
   172                                                   cur = next(self.model.parameters()).device
   173                                                   same_device = (cur == device)
   174                                                   if not same_device and cur.type == "cuda" and device.type == "cuda":
   175                                                       # If caller asked for generic "cuda", accept the current cuda:<idx>.
   176                                                       if device.index is None:
   177                                                           same_device = True
   178                                                       # If caller asked for a specific cuda:<idx>, require exact match.
   179                                                   if not same_device:
   180                                                       self.model.to(device)
   181                                           
   182                                                   inputs = self.tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
   183                                                   inputs = {k: v.to(device) for k, v in inputs.items()}
   184                                           
   185                                                   with torch.no_grad():
   186                                                       outputs = self.model(**inputs)
   187                                           
   188                                                   hs = outputs.last_hidden_state  # [B, L, H]
   189                                                   attn = inputs.get("attention_mask", None)
   190                                                   if attn is None:
   191                                                       # Fallback to unmasked mean (shouldn't happen for HF tokenizers)
   192                                                       return hs.mean(dim=1)
   193                                           
   194                                                   # Masked mean pooling: sum over real tokens / count(real tokens)
   195                                                   # This reproduces the batch_size=1 behavior (no pad tokens).
   196                                                   attn_f = attn.to(dtype=hs.dtype).unsqueeze(-1)  # [B, L, 1]
   197                                                   summed = (hs * attn_f).sum(dim=1)  # [B, H]
   198                                                   counts = attn_f.sum(dim=1).clamp_min(1.0)  # [B, 1]
   199                                                   return summed / counts

Total time: 0.55608 s
File: /home/ali/vcahce/vcache/vcache_policy/strategies/verified_splitter.py
Function: VerifiedSplitterDecisionPolicy._maxsim_similarity at line 113

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   113                                               def _maxsim_similarity(self, query: str, candidate: str, *, candidate_cache_key: str | None = None) -> float:
   114                                                   """
   115                                                   Compute MaxSim similarity in [0, 1] using the provided splitter + its embedding model.
   116                                                   """
   117        20          0.0      0.0      0.0          if self.splitter is None:
   118                                                       raise ValueError(
   119                                                           "VerifiedSplitterDecisionPolicy requires `splitter` (MaxSimSplitter) to be provided."
   120                                                       )
   121                                           
   122        20          0.0      0.0      0.0          try:
   123        20          0.0      0.0      0.0              import torch
   124        20          0.1      0.0      0.0              import torch.nn.functional as F
   125                                                   except Exception as e:
   126                                                       raise RuntimeError(
   127                                                           "VerifiedSplitterDecisionPolicy requires torch to compute MaxSim similarity."
   128                                                       ) from e
   129                                           
   130                                                   # Reuse the RL splitter's LM forward: build segment + full embeddings directly from token embeddings.
   131                                                   # This removes the expensive re-encoding of decoded segment strings via `get_embeddings_tensor`.
   132        40        543.8     13.6     97.8          query_tensor, corpus_tensor = self.splitter.split_pair_return_maxsim_tensors(
   133        20          0.0      0.0      0.0              query, candidate, cache_key_b=candidate_cache_key
   134                                                   )
   135                                           
   136                                                   # Use the tensor's device for all math
   137        20          0.0      0.0      0.0          dev = query_tensor.device
   138                                           
   139                                                   # Weights mimic MaxSimEnv: score_weights_raw = [-1e9, 0, 0] => coarse ~0, row/col ~0.5 each
   140        20          1.2      0.1      0.2          weights = torch.softmax(torch.tensor([-1e9, 0.0, 0.0], device=dev, dtype=torch.float32), dim=0)
   141        20          0.5      0.0      0.1          w_coarse, w_row, w_col = weights.tolist()
   142                                           
   143        20          2.8      0.1      0.5          coarse = F.cosine_similarity(query_tensor[-1:, :], corpus_tensor[-1:, :]).squeeze()
   144                                           
   145        20          0.2      0.0      0.0          query_sentence = query_tensor[:-1, :]
   146        20          0.1      0.0      0.0          corpus_sentence = corpus_tensor[:-1, :]
   147        20          0.0      0.0      0.0          if query_sentence.shape[0] > 0 and corpus_sentence.shape[0] > 0:
   148        20          1.6      0.1      0.3              qn = F.normalize(query_sentence, p=2, dim=-1)
   149        20          0.9      0.0      0.2              cn = F.normalize(corpus_sentence, p=2, dim=-1)
   150        20          0.9      0.0      0.2              cos = torch.mm(qn, cn.T)
   151        20          0.9      0.0      0.2              row_score = torch.max(cos, dim=1).values.mean()
   152        20          0.6      0.0      0.1              col_score = torch.max(cos, dim=0).values.mean()
   153                                                   else:
   154                                                       row_score = torch.tensor(0.0)
   155                                                       col_score = torch.tensor(0.0)
   156                                           
   157        20          1.1      0.1      0.2          raw = (w_coarse * coarse) + (w_row * row_score) + (w_col * col_score)
   158                                           
   159                                                   # Map [-1, 1] -> [0, 1] and clip
   160        20          1.2      0.1      0.2          s01 = float(torch.clamp((raw + 1.0) / 2.0, 0.0, 1.0).item())
   161        20          0.0      0.0      0.0          return s01

Total time: 0.834669 s
File: /home/ali/vcahce/vcache/vcache_policy/strategies/verified_splitter.py
Function: VerifiedSplitterDecisionPolicy._select_nn_by_maxsim at line 163

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   163                                               def _select_nn_by_maxsim(
   164                                                   self, prompt: str
   165                                               ) -> Tuple[Optional[EmbeddingMetadataObj], Optional[float]]:
   166                                                   """
   167                                                   Select the nearest-neighbor metadata object using MaxSim similarity.
   168                                           
   169                                                   Returns:
   170                                                       (best_metadata, best_similarity) where similarity is in [0, 1].
   171                                                   """
   172        20          0.0      0.0      0.0          if self.cache is None:
   173                                                       return None, None
   174                                           
   175        20          0.0      0.0      0.0          candidates: list[EmbeddingMetadataObj] = []
   176                                           
   177        20          0.0      0.0      0.0          if self.candidate_selection == "all":
   178                                                       candidates = self.cache.get_all_embedding_metadata_objects()
   179        20          0.0      0.0      0.0          elif self.candidate_selection == "top_k":
   180        20        277.6     13.9     33.3              knn = self.cache.get_knn(prompt=prompt, k=max(1, int(self.candidate_k)))
   181        40          0.0      0.0      0.0              for _db_score, embedding_id in knn:
   182        20          0.0      0.0      0.0                  try:
   183        20          0.2      0.0      0.0                      candidates.append(self.cache.get_metadata(embedding_id=embedding_id))
   184                                                           except Exception:
   185                                                               continue
   186                                                   else:
   187                                                       raise ValueError(
   188                                                           f"Unknown candidate_selection={self.candidate_selection!r}. Use 'top_k' or 'all'."
   189                                                       )
   190                                           
   191        20          0.0      0.0      0.0          if not candidates:
   192                                                       return None, None
   193                                           
   194        20          0.0      0.0      0.0          best_meta: Optional[EmbeddingMetadataObj] = None
   195        20          0.0      0.0      0.0          best_s: float = -1.0
   196                                           
   197        40          0.0      0.0      0.0          for meta in candidates:
   198        20          0.0      0.0      0.0              cached_prompt = getattr(meta, "prompt", "") or ""
   199        20          0.0      0.0      0.0              if not cached_prompt:
   200                                                           # Can't compute MaxSim without cached prompt text; skip.
   201                                                           continue
   202        20          0.0      0.0      0.0              try:
   203        40        556.6     13.9     66.7                  s = self._maxsim_similarity(
   204        20          0.0      0.0      0.0                      prompt,
   205        20          0.0      0.0      0.0                      cached_prompt,
   206        20          0.0      0.0      0.0                      candidate_cache_key=f"emb:{getattr(meta, 'embedding_id', None)}",
   207                                                           )
   208                                                       except Exception as e:
   209                                                           self.logger.warning(f"MaxSim similarity failed for one candidate: {e}")
   210                                                           continue
   211        20          0.0      0.0      0.0              if s > best_s:
   212        20          0.0      0.0      0.0                  best_s = s
   213        20          0.0      0.0      0.0                  best_meta = meta
   214                                           
   215        20          0.0      0.0      0.0          if best_meta is None:
   216                                                       return None, None
   217        20          0.0      0.0      0.0          return best_meta, best_s

Total time: 0.889416 s
File: /home/ali/vcahce/vcache/vcache_policy/strategies/verified_splitter.py
Function: VerifiedSplitterDecisionPolicy.process_request at line 219

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   219                                               def process_request(
   220                                                   self, prompt: str, system_prompt: Optional[str], id_set: int
   221                                               ) -> Tuple[bool, str, EmbeddingMetadataObj]:
   222        20          0.0      0.0      0.0          if self.inference_engine is None or self.cache is None:
   223                                                       raise ValueError("Policy has not been setup")
   224                                           
   225                                                   # If cache is empty, this will return (None, None)
   226        20        834.9     41.7     93.9          nn_metadata, similarity_score = self._select_nn_by_maxsim(prompt)
   227        20          0.0      0.0      0.0          if nn_metadata is None or similarity_score is None:
   228                                                       response = self.inference_engine.create(prompt=prompt, system_prompt=system_prompt)
   229                                                       self.cache.add(prompt=prompt, response=response, id_set=id_set)
   230                                                       return False, response, EmbeddingMetadataObj(embedding_id=-1, response="")
   231                                           
   232        20         53.1      2.7      6.0          action = self.bayesian.select_action(similarity_score=similarity_score, metadata=nn_metadata)
   233                                           
   234        20          0.0      0.0      0.0          match action:
   235        20          0.0      0.0      0.0              case _Action.EXPLOIT:
   236                                                           return True, nn_metadata.response, nn_metadata
   237        20          0.0      0.0      0.0              case _Action.EXPLORE:
   238        20          0.1      0.0      0.0                  response = self.inference_engine.create(prompt=prompt, system_prompt=system_prompt)
   239        40          1.2      0.0      0.1                  self.__update_cache(
   240        20          0.0      0.0      0.0                      response=response,
   241        20          0.0      0.0      0.0                      nn_metadata=nn_metadata,
   242        20          0.0      0.0      0.0                      similarity_score=similarity_score,
   243        20          0.0      0.0      0.0                      embedding_id=nn_metadata.embedding_id,
   244        20          0.0      0.0      0.0                      prompt=prompt,
   245        20          0.0      0.0      0.0                      label_id_set=id_set,
   246                                                           )
   247        20          0.0      0.0      0.0                  return False, response, nn_metadata

